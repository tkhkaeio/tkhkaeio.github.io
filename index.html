<!DOCTYPE html>
<html lang="en">
<head>
<title>Take Ohkawa</title>
<meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<meta name="author" content="Take Ohkawa">
<meta name="description" content="Take Ohkawa's Homepage">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!-- <meta name="google-site-verification" content="googlef77166d9f74b3a8e.html"> -->
<meta name="google-site-verification" content="lrotvHHzOzWJnzATjx0FhDknYrDysUQuDse7oJyruA4" />
<link rel="stylesheet" type="text/css" href="stylesheet.css">
<!-- <link rel="icon" type="image/png" href="XXX"> -->
<script type="text/javascript">
        function load_text(filename, id){
            window.addEventListener('DOMContentLoaded', function(){
                fetch(filename)
                    .then(response => response.text())
                    .then(data => {
                    const file_area = document.getElementById(id);
                    file_area.innerHTML = data.replace(/\n/g, "<br>");
                    //file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&emsp;");
                    file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&nbsp;");
                });
            });
        };
</script>
</head>

<body>
<table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
                <td style="padding: 0% 2.5% 0% 2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Take Ohkawa</name>
                </p>
                <p>
                    Hi, I'm a Research Scientist at <a href="https://www.airoa.org/">AIRoA</a>, working on Vision-Language-Action (VLA) systems. I also work as a Cooperative Research Fellow at the Institute of Industrial Science, UTokyo.
                    <br><br>

                    I received my PhD from UTokyo in 2025, advised by Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a>.
                    During my PhD journey, I served as <a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2024">Google PhD Fellow</a> in 2024.                    
                    I joined Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a> lab at ETH Zurich in 2023, and Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> lab at CMU in 2021. 
                    <!-- <br> -->
                    I interned at Meta Reality Labs to work with Dr. <a href="https://sites.google.com/view/takaaki-shiratori/home">Takaaki Shiratori</a> and Dr. <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a> in Pittsburgh in 2024, and with Dr. <a href="https://kunhe.github.io/">Kun He</a>,  Dr. <a href="https://fadimesener.github.io/">Fadime Sener</a>, and Dr. <a href="https://thodan.github.io/">Tomas Hodan</a> in Redmond in 2022.
                    I worked closely with Dr. <a href="http://yoshitakaushiku.net/">Yoshitaka Ushiku</a> and Dr. <a href="https://atsushihashimoto.github.io/cv/">Atsushi Hashimoto</a> at OMRON SINIC X.
                    <br><br>
                    Fortunately, 
                    I received EgoVis Distinguished Paper Award at CVPR 2025, and competitive fellowships from Google (2024), Microsoft Research Asia (2023), and ETH Zurich Leading House Asia (2023). 
                    I served as Principal Investigator of JST ACT-X Project (2020-2023) and JSPS Research Fellow (DC1) (2022-2024).
                    <br>
                </p>
                <!-- <p>
                    I attained my master's degree with early completion in 1.5 years at The University of Tokyo under the supervision of Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a> and Prof. <a href="https://lab.rekimoto.org/members/rekimoto/">Jun Rekimoto</a>. 
                    Prior to that, I received my bachelor's degree with early completion in 3 years at the Tokyo Institute of Technology under the guidance of Prof. <a href="https://mmai.tech/">Nakamasa Inoue</a>.
                    <br>
                </p> -->
                <p style="text-align:left">
                    E-mail: tohkawa [at] contact [at] gmail [dot] com<br>
                    <!-- <a href="mailto:xxx@x.com">Email</a> &nbsp/&nbsp -->                    
                    <!-- <a href="xxx.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?hl=en&user=WNIxd2UAAAAJ&hl">Google
                    Scholar</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/tkhkaeio/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/tkhkaeio">X (Twitter)</a> &nbsp/&nbsp
                    <a href="https://drive.google.com/file/d/1xERt3NKcF-Zj68Sv-4CgkC7ai_uSYnLl/view?usp=sharing">CV</a>
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://github.com/xxxx/">Github</a> -->
                </p>
                </td>
                <!-- Profile image -->
                <td style="padding:2.5%;width:35%;max-width:35%;">                  
                    <img
                    style="width:100%;max-width:100%;min-width:100%"
                    src="./photos/2025_portait_wide.JPG"
                    alt="profile photo"                    
                    class="hoverZoomLink">         
                    <!-- src="https://user-images.githubusercontent.com/28190044/270403272-9c544d58-e70a-40ca-a546-d233e450a350.jpeg"                     -->
                </td>                
            </tr>
            </tbody>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0" style="margin-top:0px;">
            <tr>
                <!-- airoa -->
                <td width="10%" valign="middle" align="center">
                <!-- <img src="https://gist.github.com/user-attachments/assets/5627081f-848d-45c4-80b8-1a78f6008394" width="100"> -->
                <img src="./assets/airoa_logo.png" width="100">
                <figcaption>AIRoA</figcaption>
                </td>                     
                <!-- utokyo -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/77ab1572-37b7-4991-8e08-d71d3cea1717" width="110">
                <figcaption>PhD'21-'25</figcaption>
                </td>         
                <!-- meta -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/user-attachments/assets/802d5497-acf1-42fb-ae47-a177a2ff21ec" width="100">
                <figcaption>Intern'22,'24</figcaption>
                </td>            
                <!-- google -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/user-attachments/assets/4936c7db-f091-4bdd-b22d-4f05fb801773" width="90">
                <figcaption>PhD Fellow'24</figcaption>
                </td>
                <!-- eth zurich -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/6968d9e1-36c3-4f44-8683-f53485001620" width="120">
                <figcaption>Visitor'23</figcaption>
                </td>
                <!-- cmu ri -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/39f4ce4f-964e-4e25-b6d1-ee81cfb7a0fc" width="95">
                <figcaption>Visitor'21</figcaption>
                </td>                
                <!-- miscrosoft -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/15eb8318-fdb5-4b0b-b311-fb7b706e693f" width="140">
                <figcaption>PhD Fellow'23</figcaption>
                </td>
                <!-- omron -->
                <!-- <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/286ce042-5a8b-4344-a5ee-e5d32451d311" width="100">
                <figcaption>Intern'23,'20</figcaption>
                </td>                      -->
            </tr>
            </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px 20px 10px 20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                    <b>[Dec 2025]</b> Invited to give a talk at <a href="https://gdg.community.dev/events/details/google-gdg-ai-for-science-japan-presents-shi-jue-karashen-ti-xing-wochi-tsuaihe-qiao-zhi-nadong-zuo-no3ci-yuan-li-jie/">Google Developer Groups AI for Science - Japan</a>!<br>
                    <b>[Nov 2025]</b> Paper <a href="./projects/25-dfmamba/index.html">"DF-Mamba"</a> got accepted to WACV 2026!<br>
                    <b>[Sep 2025]</b> Five papers <a href="https://arxiv.org/abs/2509.23888">"AssemblyHands-X"</a>,
                    <a href="https://arxiv.org/abs/2510.00506">"AffHandGen"</a>, <a href="https://arxiv.org/abs/2509.16949">"RGB2Event"</a>, <a href="https://hands-workshop.org/files/2025/DF-Mamba.pdf">"DF-Mamba"</a>, <a href="./projects/25-scgen/index.html">"SCGen"</a> got accepted to ICCVW 2025!<br>
                    <b>[Jul 2025]</b> <a href="./projects/25-thesis/index.html">Defended my PhD</a>&#129489;&#8205;&#127891;!<br>
                    <b>[Jun 2025]</b> Paper <a href="./projects/25-scgen/index.html">"SCGen"</a> got accepted to ICCV 2025!<br>
                    <b>[Jun 2025]</b> Received <a href="https://egovis.github.io/awards/2023_2024/">EgoVis Distinguished Paper Award</a>, CVPR 2025!<br>
                    <b>[Apr 2025]</b> 9th <a href="https://hands-workshop.org/">HANDS</a> workshop proposal got aceepted to ICCV 2025! See you in Hawaii!<br>
                    <b>[Apr 2025]</b> Invited to give a talk at <a href="https://vision.cs.illinois.edu/vision_website/#five">UIUC Vision Seminar</a>!<br>
                    <b>[Jan 2025]</b> Paper <a href="https://arxiv.org/abs/2502.15251">"SiMHand"</a> got accepted to ICLR 2025.<br>
                    <b>[Nov 2024]</b> Received <a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2024">Google PhD Fellowship</a> in Machine Perception!<br>
                    <b>[Oct 2024]</b> Paper <a href="https://arxiv.org/abs/2311.16444">"Exo2EgoDVC"</a> got accepted to WACV 2025.<br>
                    <b>[Jul 2024]</b> Three papers <a href="https://arxiv.org/abs/2403.16428">"HANDS'23 Analysis"</a>, <a href="https://arxiv.org/abs/2311.17366">"GHTT"</a>, <a href="https://arxiv.org/abs/2409.09714">"HandCLR"</a> got accepted to ECCV 2024.<br>                             
                    <details>
                    <summary>Past updates</summary>
                    <b>[Jun 2024]</b> Started an internship at <a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a> @Pittsburgh.<br>       
                    <b>[Apr 2024]</b> Two papers <a href="https://arxiv.org/abs/2403.04381">"S2DHand"</a> and <a href="https://arxiv.org/abs/2311.16444">"Exo2EgoDVC"</a> got accepted to CVPR 2024.<br>
                    <b>[Apr 2024]</b> 8th <a href="https://hands-workshop.org/">HANDS</a> workshop proposal got aceepted to ECCV 2024 with Dr. <a href="https://www.mu4yang.com/">Linlin</a> at CUC. See you in Milan!<br>
                    <b>[Apr 2024]</b> Gave an invited presenatation at <a href="https://sites.google.com/ut-vision.org/aspire-hcvm/">JST ASPIRE HCVM</a> workshop, UTokyo-IIS.<br>
                    <b>[Jul 2023]</b> Paper <a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">"Survey on 3D Hand Pose Estimation"</a> got accepted to IJCV.<br>
                    <b>[Jul 2023]</b> Started working as a Visiting Researcher at <a href="https://cvg.ethz.ch/">CVG Group</a>, ETH Zurich.<br>    
                    <b>[Jun 2023]</b> Invited to give a talk at <a href="https://cvml.comp.nus.edu.sg/">CVML Group</a>, NUS.<br>
                    <b>[Apr 2023]</b> Research proposals got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X Acceleration Phase</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>.<br>
                    <b>[Mar 2023]</b> Host 7th <a href="https://sites.google.com/view/hands2023/">HANDS</a> workshop at ICCV 2023 with Prof. <a href="https://www.comp.nus.edu.sg/~ayao/">Angela</a> at NUS. See you in Paris!<br>
                    <b>[Feb 2023]</b> Paper <a href="https://assemblyhands.github.io/">"AssemblyHands"</a> got accepted to CVPR 2023.<br>
                    <b>[Jul 2022]</b> Paper <a href="https://arxiv.org/abs/2203.08344">"Hand State Estimation in the Wild"</a> got accepted to ECCV 2022.<br>       
                    <b>[Feb 2022]</b> Received UTokyo-IIS Research Collaboration Initiative Award with <a href="https://twitter.com/tkhkaeio/status/1507297146772799491?s=20&t=0IM8HYvhrWsCrqv9SnAJzw">Oculus Quests</a>!<br>        
                    <b>[Sep 2021]</b> Obtained M.A.S. for 1.5 years (<a>Early Graduation</a>), UTokyo.<br>
                    <b>[Jun 2021]</b> Paper <a href="http://arxiv.org/abs/2107.02718">"Domain Adaptation of Hand Segmentation"</a> got accepted to IEEE Access 2021.<br>
                    <b>[Oct 2020]</b> Research proposal got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X</a>.<br>
                    <b>[Oct 2020]</b> Paper <a href="https://arxiv.org/abs/2003.00187">"Augmented Cyclic Consistency Regularization"</a> got accepted to ICPR2020.<br>
                    <b>[Apr 2020]</b> Joined <a href="https://www.ut-vision.org/sato-lab/">Sato/Sugano Lab</a> at UTokyo.<br>
                    <b>[Mar 2020]</b> Obtained B.E. for 3 years (<a>Early Graduation</a>) at TokyoTech.<br>
                    <b>[Oct 2019]</b> Gifted NVIDIA RTX 2080Ti from <a href="https://twitter.com/tkhkaeio/status/1186167767415672833?s=20">Yu Darvish</a>, a Japanese MLB player who I respect the most!<br>
                    <b>[Oct 2019]</b> Joined <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Inoue Lab</a> at TokyoTech.<br>
                    </details>
                </p>

                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px 20px 20px 20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                    I'm interested in <b>computer vision, machine learning, and robotics for embodied AI</b>, aiming to enable agents that can perceive, understand, and generate interactions in the real world. My research interest includes:
                    <ul>
                    <li>3D modeling of human interactions with hands, bodies, and objects</li>
                    <li>Egocentric perception and video understanding</li>
                    <li>Generative and diffusion models</li>
                    <li>Vision-Language-Action (VLA) systems</li>
                    </ul>
                    <br>
                    <span class="text-accent">
                    If you're interested in working with me, please visit my <a href="./contact/recruitment.html">contact page</a> and feel free to reach out!
                    </span> 

                    <!-- <ul>
                        <li> <i>3D reconstruction and tracking</i>, advancing human understanding through geometry, pose, surface, and contact modeling.</li>
                        <li> <i>Strong prior modeling</i>, leveraging diffusion models, large-scale visual pre-training, and state-space modeling (e.g., Mamba) to enable generative and adaptive representations.  </li>
                        <li> <i>Multi-modal video understanding</i>, integrating vision-language reasoning to connect perception with temporal dynamics, activity recognition, and semantic interpretation.  </li>
                        <li> <i>Egocentric vision</i>, modeling first-person videos to capture dynamic interactions fundamental to AR/VR and robotics. </li>
                        <li> <i>Transfer learning</i>, designing adaptive visual systems that generalize across environments, tasks, and modalities.  </li>
                    </ul> -->
                </p>
                </td>
            </tr>
            </tbody></table>        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:middle">
                <subheading>Publications</subheading>
                </td>
            </tr>
        </tbody>
        </table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <!-- <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/XXX/figs/XXX.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/tbd">
                    <papertitle>XX</papertitle>
                    </a>
                    <br>
                    <b>Takehiko Ohkawa</b>
                    <br>
                    <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2025<br>
                    <a href="xxxxx">[Project]</a> /
                    <a href="https://arxiv.org/abs/tbd">[Paper (TBD)]</a>
                    <a href="./projects/25-xxxx/bibtex.txt">[BibTex]</a>                
                    <label class="open" for="pop-up-xxx"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-xxx" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-xxx">×</label>
                        <div class="bibtex" id="bibtex-25-xxx"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-xxx/bibtex.txt';
                        load_text(filename, 'bibtex-25-xxx');
                        </script>
                    </div>
                    </div>
                    <p></p>
                    <p>
                        xxx
                    </p>
                </td>
            </tbody> -->

            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/25-dfmamba/figs/dfmamba_teaser.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://hands-workshop.org/files/2025/DF-Mamba.pdf">
                    <papertitle>DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions</papertitle>
                    </a>
                    <br>
                    Yifan Zhou*, <b>Takehiko Ohkawa*</b>, Guwenxiao Zhou, Kanoko Goto, Takumi Hirose, Yusuke Sekikawa, and Nakamasa Inoue (*equal contribution)
                    <br>
                    <em>IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>)</em>, 2026<br>
                    <a href="https://hands-workshop.org/workshop2025.html">HANDS</a>, <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    <a href="./projects/25-dfmamba/index.html">[Project]</a>
                    <!-- <a href="tbd">[Paper]</a> -->
                    <a href="http://arxiv.org/abs/2512.02727">[Paper]</a>
                    <label class="open" for="pop-up-25-dfmamba"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-25-dfmamba" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-25-dfmamba">×</label>
                        <div class="bibtex" id="bibtex-25-dfmamba"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-dfmamba/bibtex.txt';
                        load_text(filename, 'bibtex-25-dfmamba');
                        </script>
                    </div>
                    </div>
                    <p></p>
                    <p>
                        We propose a novel and powerful framework for visual feature extraction in 3D hand pose estimation using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues via Mamba's nature of selective state modeling and deformable state scanning.
                    </p>
                </td>
            </tbody>

            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/25-ahx/figs/ahx_teaser.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2509.23888">
                    <papertitle>AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities</papertitle>
                    </a>
                    <br>
                    Tatsuro Banno, <b>Takehiko Ohkawa</b>, Ruicong Liu, Ryosuke Furuta, and Yoichi Sato
                    <br>
                    <a href="https://hands-workshop.org/workshop2025.html">HANDS</a>, <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2509.23888">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a>                 -->
                    <p></p>
                    <p>
                        We present AssemblyHands-X, the first markerless 3D hand-body benchmark for bimanual activities, designed to study the effect of hand-body coordination for action recognition.
                    </p>
                </td>
            </tbody>

            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/25-affordhand/figs/afford_teaser.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2510.00506">
                    <papertitle>Affordance-Guided Diffusion Prior for 3D Hand Reconstruction</papertitle>
                    </a>
                    <br>                    
                    Naru Suzuki, <b>Takehiko Ohkawa</b>, Tatsuro Banno, Jihyun Lee, Ryosuke Furuta, and Yoichi Sato
                    <br>
                    <a href="https://hands-workshop.org/workshop2025.html">HANDS</a>, <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2510.00506">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a>                 -->
                    <p></p>
                    <p>
                        We propose a generative prior for hand pose refinement guided by affordance-aware textual descriptions of hand-object interactions. Our method employs a diffusion-based generative model that learns the distribution of plausible hand poses conditioned on affordance descriptions, which are inferred from VLMs.
                    </p>
                </td>
            </tbody>
            
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/25-rgb2event/figs/rgb2event_teaser.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2509.16949">
                    <papertitle>Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation</papertitle>
                    </a>
                    <br>
                    Ruicong Liu, <b>Takehiko Ohkawa</b>, Tze Ho Elden Tse, Mingfang Zhang, Angela Yao, and Yoichi Sato
                    <br>
                    <a href="https://hands-workshop.org/workshop2025.html">HANDS</a>, <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2509.16949">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a>                 -->
                    <p></p>
                    <p>
                        This paper presents RPEP, the first pre-training method for event-based 3D hand pose estimation using labeled RGB images and unpaired, unlabeled event data. 
                    </p>
                </td>
            </tbody>

            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/25-scgen/figs/scgen.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2509.23393">
                    <papertitle>Generative Modeling of Shape-Dependent Self-Contact Human Poses</papertitle>
                    </a>
                    <br>
                    <b>Takehiko Ohkawa</b>, Jihyun Lee, Shunsuke Saito, Jason Saragih, Fabian Prado, Yichen Xu, Shoou-I Yu, Ryosuke Furuta, Yoichi Sato, and Takaaki Shiratori<br>
                    <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2025<br>
                    <a href="https://i-hfm-2025.github.io/I-HFM-2025/">I-HFM</a> & <a href="https://hands-workshop.org/workshop2025.html">HANDS</a>, <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    <!-- <br> -->
                    <a href="./projects/25-scgen/index.html">[Project]</a>
                    <a href="https://arxiv.org/abs/2509.23393">[Paper]</a>
                    <!-- <a href="https://github.com/facebookresearch/SCGen">[Data &Code]</a> -->
                    <a href="https://drive.google.com/file/d/1Meh5yZxmj_K0XckX8qgQcCYmaBEYMMJv/view?usp=sharing">[Poster]</a>
                    <label class="open" for="pop-up-scgen"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-scgen" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-scgen">×</label>
                        <div class="bibtex" id="bibtex-25-scgen"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-scgen/bibtex.txt';
                        load_text(filename, 'bibtex-25-scgen');
                        </script>
                    </div>
                    </div>
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                        We introduce the first extensive self-contact dataset with precise body shape registration, Goliath-SC, consisting of 383K self-contact poses across 130 subjects. Using this dataset, we propose generative modeling of a self-contact prior conditioned by body shape parameters, based on a body-part-wise latent diffusion with self-attention.
                    </p>
                </td>   
            </tbody>
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <img src="projects/25-thesis/figs/thesis_teaser_square.png" width="180" height="180">
                    </div>
                </td>
                <!-- <td style="padding:10px 20px 20px 20px;width:75%;vertical-align:middle"> -->
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="./projects/25-thesis/index.html">
                    <papertitle>Visual Understanding of Human Hands in Interactions</papertitle>
                    </a><br>
                    <b>Takehiko Ohkawa</b><br>
                    <em>Doctoral Dissertation</em>, 2025<br>
                    <!-- <br> -->
                    <a href="./projects/25-thesis/index.html">[Overview]</a>
                    <a href="./projects/25-thesis/pdf/thesis_full.pdf">[Paper]</a>
                    <a href="https://speakerdeck.com/tkhkaeio/phd-defense-2025-visual-understanding-of-human-hands-in-interactions">[Slides]</a>
                    <label class="open" for="pop-up-thesis"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-thesis" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-thesis">×</label>
                        <div class="bibtex" id="bibtex-25-thesis"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-thesis/bibtex.txt';
                        load_text(filename, 'bibtex-25-thesis');
                        </script>
                    </div>
                    </div>
                </td>                
            </tbody>            
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/278592ae-c87c-47a6-9793-b92d92a244ce"
                        width="180" height="auto">
                        <!-- class="img-fit"> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://hands-workshop.org/">
                    <papertitle>HANDS Workshops: Observing and Understanding Hands in Action</papertitle>
                    </a><br>                    
                    Contributed as an organizer and challenge committee<br>                    
                    <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    Past editions: <a href="https://hands-workshop.org/workshop2024.html">[ECCV2024]</a> / <a href="https://sites.google.com/view/hands2023/">[ICCV2023]</a> <br>
                    <details>                             
                        <summary>Details</summary>                                                   
                        Hosted challenges:<br>
                        <a href="https://codalab.lisn.upsaclay.fr/competitions/19885#results">AssemblyHands-S2D</a>@ECCV2024: Dual-view hand pose<br>
                        <a href="https://codalab.lisn.upsaclay.fr/competitions/15149#results">AssemblyHands</a>@ICCV2023: Single-view hand pose<br>
                    </details>
                    <p></p>
                    <p>
                        Our HANDS workshop will gather vision researchers working on perceiving hands performing actions, including 2D & 3D hand detection, segmentation, pose/shape estimation, tracking, etc. We will also cover related applications including gesture recognition, hand-object manipulation analysis, hand activity understanding, and interactive interfaces.
                    </p>
                </td>
            </tbody>
            
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <!-- <img src="https://gist.github.com/user-attachments/assets/88610931-68ba-4af7-b319-778e54f55116" width="180" height="180"> -->
                     <img src="projects/25-emosign/figs/emosign.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2505.17090">
                    <papertitle>EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language</papertitle>
                    </a>
                    <br>
                    Phoebe Chua*, Cathy Mengying Fang*, <b>Takehiko Ohkawa</b>, Raja Kushalnagar, Suranga Nanayakkara, and Pattie Maes (*equal contribution)
                    <br>
                    Preprint<br>
                    <!-- <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2024<br> -->
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2505.17090">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                        We introduce EmoSign, the first sign video dataset containing sentiment and emotion labels for 200 American Sign Language (ASL) videos with open-ended descriptions of emotion cues.
                        Alongside the annotations, we include baseline models for sentiment and emotion classification.
                        
                    </p>
                </td>   
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/user-attachments/assets/f6108781-2808-430e-8005-07e18ca7f4b9"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2502.15251">
                    <papertitle>SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training</papertitle>
                    </a>
                    <br>                    
                    Nie Lin*, <b>Takehiko Ohkawa*</b>, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, and Yoichi Sato (*equal contribution)
                    <br>
                    <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2025<br>
                    <a href="https://hands-workshop.org/workshop2024.html">HANDS</a>, <em>European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2024<br>                    
                    Invited Oral Presentation at <em>Meeting on Image Recognition and Understanding (<b>MIRU</b>)</em>, 2025<br>
                    <a href="./projects/25-simhand/index.html">[Project]</a>
                    <a href="https://arxiv.org/abs/2502.15251">[Paper]</a>
                    <a href="https://github.com/ut-vision/SiMHand">[Code]</a>
                    <a href="https://drive.google.com/file/d/1juP-FlV9o5pTYVi7mviTU4DMcKiAiovB/view?usp=sharing">[Poster]</a>
                    <!-- <a href="https://openreview.net/forum?id=xxx">[OpenReview]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <label class="open" for="pop-up-simhand"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-simhand" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-simhand">×</label>
                        <div class="bibtex" id="bibtex-25-simhand"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-simhand/bibtex.txt';
                        load_text(filename, 'bibtex-25-simhand');
                        </script>
                    </div>
                    </div>

                    <p></p>
                    <p>
                        We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SiMHand.
                    </p>
                </td>
            </tbody>
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/19d8b429-5ea1-4ad2-8aca-04e15fbc8609"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2311.16444">
                    <papertitle>Exo2EgoDVC: Dense Video Captioning of Egocentric Human Activities Using Web Instructional Videos</papertitle>
                    </a>
                    <br>
                    <b>Takehiko Ohkawa</b>, Takuma Yagi, Taichi Nishimura, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato
                    <br>
                    <em>IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>)</em>, 2025<br>
                    <a href="https://sites.google.com/view/lpvl-cvpr2024/home">LPVL</a>,                    
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (<b>CVPRW</b>)</em>, 2024<br>
                    <!-- <br> -->
                    <a href="./projects/25-egodvc/index.html">[Project]</a>
                    <a href="https://arxiv.org/abs/2311.16444">[Paper]</a>
                    <a href="https://github.com/ut-vision/Exo2EgoDVC">[Data & Code]</a>
                    <a href="https://youtu.be/rJvuLFrGBGQ">[Video]</a>
                    <label class="open" for="pop-up-7"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-7" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-7">×</label>
                        <div class="bibtex" id="bibtex-25-egodvc"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-egodvc/bibtex.txt';
                        load_text(filename, 'bibtex-25-egodvc');
                        </script>
                    </div>
                    </div>
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We present EgoYC2, a novel benchmark for cross-view knowledge transfer of dense video captioning, adapting models from web instructional videos with exocentric views to an egocentric view.
                    </p>
                </td>
            </tbody>
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/b3584bf6-1e28-46a7-9b27-27b45b889795"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2403.16428">
                    <papertitle>Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects</papertitle>
                    </a>
                    <br>
                    Zicong Fan*, <b>Takehiko Ohkawa*</b>, Linlin Yang*, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, and Angela Yao (*equal contribution)
                    <br>
                    <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2024<br>
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2403.16428">[Paper]</a>
                    <a href="https://drive.google.com/file/d/1eFyXGjFeB-aASpWaCFsZ7VOK1nQMAtM-/view?usp=drive_link">[Poster]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We present a comprehensive summary of the HANDS23 challenge using the AssemblyHands and ARCTIC datasets. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. 
                    </p>
                </td>
            </tbody>       
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/cb58ae2c-68e1-4766-95fd-401e13923634"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2311.17366">
                    <papertitle>Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling</papertitle>
                    </a>
                    <br>
                    Yilin Wen, Hao Pan, <b>Takehiko Ohkawa</b>, Lei Yang, Jia Pan, Yoichi Sato, Taku Komura, and Wenping Wang
                    <br>
                    <a href="https://hands-workshop.org/workshop2024.html">HANDS</a>, <em>European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2024<br>                    
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2311.17366">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We present a novel framework that concurrently tackles hand action recognition and 3D future hand motion prediction.
                    </p>
                </td>
            </tbody>     
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/40ae7198-1e27-4d12-a10c-dbc55e4e30b3"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2403.04381">
                    <papertitle>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</papertitle>
                    </a>
                    <br>
                    Ruicong Liu, <b>Takehiko Ohkawa</b>, Mingfang Zhang, Yoichi Sato
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2024<br>                    
                    Invited Poster Presentation at <em><a href="https://egovis.github.io/cvpr24/">EgoVis Workshop</a>, <b>CVPRW</b></em>, 2024<br>
                    Invited Oral Presentation at <em>Forum on Information Technology (<b>FIT</b>)</em>, 2024<br>
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2403.04381">[Paper]</a>                    
                    <a href="https://github.com/ut-vision/S2DHand">[Code]</a>
                    <label class="open" for="pop-up-6"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-6" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-6">×</label>
                        <div class="bibtex" id="bibtex-24-s2dhand"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/24-s2dhand/bibtex.txt';
                        load_text(filename, 'bibtex-24-s2dhand');
                        </script>
                    </div>
                    </div>

                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We propose a novel Single-to-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views.
                    </p>
                </td>
            </tbody>            
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <!-- <img src="https://user-images.githubusercontent.com/28190044/224896516-0c474c0b-a76a-4de0-b539-fc4a93b048d5.png" width="180" height="180"> -->
                <img src="https://user-images.githubusercontent.com/28190044/228722223-dc127074-61e5-4b37-92dc-058a73780706.gif" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2304.12301">
                <papertitle>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, and Cem Keskin
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023<br>
                <em><a class="text-accent" href="https://egovis.github.io/awards/2023_2024/">EgoVis Distinguished Paper Award</a>, <b>CVPR</b></em>, 2025<br>
                Invited Oral Presentation at <em><a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/home?authuser=0">Ego4D & EPIC Workshop</a>, <b>CVPRW</b></em>, 2023<br>
                Poster Presentation at <em>International Computer Vision Summer School (<b>ICVSS</b>)</em>, 2023<br>
                <a href="https://sites.google.com/view/hands2023/challenges/assemblyhands">HANDS Workshop Benchmark Dataset</a>, <em><b>ICCVW</b></em>, 2023<br>
                <a href="https://arxiv.org/abs/2304.12301">[Paper]</a>
                <a href="https://assemblyhands.github.io/">[Project]</a>                
                <!-- <a href="https://codalab.lisn.upsaclay.fr/competitions/15149#results">[Leaderboard]</a> -->
                <a href="https://github.com/facebookresearch/assemblyhands-toolkit">[Code & Data]</a>                
                <label class="open" for="pop-up-5"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-5" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-5">×</label>
                    <div class="bibtex" id="bibtex-23-assemblyhands"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/23-assemblyhands/bibtex.txt';
                    load_text(filename, 'bibtex-23-assemblyhands');
                    </script>
                </div>
                </div>

                <p>
                We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of challenging hand-object interactions from egocentric videos.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/172274533-aaaf7014-a379-4025-a6e2-ad9e96da7b75.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">
                <papertitle>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em>International Journal of Computer Vision (<b>IJCV</b>)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2206.02257">[Paper]</a>
                <a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">[Springer]</a>
                <a href="https://drive.google.com/file/d/15gHEeyeCuzFyGYBMK971U_524M55iR1d/view?usp=share_link">[Slides]</a>
                <label class="open" for="pop-up-4"><a>[BibTex]</a></label>                
                <input type="checkbox" id="pop-up-4" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-4">×</label>
                    <div class="bibtex" id="bibtex-22-hpe-survey"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/22-hpe-survey/bibtex.txt';
                    load_text(filename, 'bibtex-22-hpe-survey');
                    </script>
                </div>
                </div>

                <p>
                We present a systematic review of 3D hand pose estimation from the perspective of efficient annotation and learning. 3D hand pose estimation has been an important research area owing to its potential to enable various applications, such as video understanding, AR/VR, and robotics. 
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/159080951-fe11090c-664c-4ce1-bf11-67ae2184b7f9.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2203.08344">
                <papertitle>Domain Adaptive Hand Keypoint and Pixel Localization in the Wild</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, and Yoichi Sato<br>
                <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022<br>
                Invited Poster Presentation at <em><a href="https://sites.google.com/view/hands2022/home">HANDS</a> and <a href="https://sites.google.com/view/egocentric-hand-body-activity/home?authuser=0">HBHA</a> workshops, <b>ECCVW</b></em>, 2022<br>
                Invited Oral Presentation at <em>Meeting on Image Recognition and Understanding (<b>MIRU</b>)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2203.08344">[Paper]</a>
                <a href="./projects/22-hand-ps-da/">[Project]</a>
                <a href="https://drive.google.com/file/d/1Ub_-653uSLYb70YeeXpE67H2MDWOrr1z/view?usp=share_link">[Slides]</a>
                <label class="open" for="pop-up-3"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-3" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-3">×</label>
                    <div class="bibtex" id="bibtex-22-hand-ps-da"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/22-hand-ps-da/bibtex.txt';
                    load_text(filename, 'bibtex-22-hand-ps-da');
                    </script>
                </div>
                </div>
                <p>
                We tackled domain adaptation of hand keypoint regression and hand segmentation to in-the-wild egocentric videos with new imaging conditions (e.g., Ego4D).
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/156375706-66ba3fe7-25f5-4405-a987-e787b993b362.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2202.13941">
                <papertitle>Background Mixup Data Augmentation for Hand and Object-in-Contact Detection</papertitle>
                </a>
                <br>
                Koya Tango, <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em><a href="https://sites.google.com/view/hands2022/home">HANDS</a>, European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2022

                <br>
                <!-- <a href="xxxxx">[Project]</a> / -->
                <a href="https://arxiv.org/abs/2202.13941">[Paper]</a>
                <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <p></p>
                <p>
                We propose Background Mixup augmentation that leverages data-mixing regularization for hand-object detection while avoiding unintended effect produced by naive Mixup.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src="https://user-images.githubusercontent.com/28190044/124936861-dcfd0e80-e041-11eb-98bb-64ae613aa74c.gif" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://arxiv.org/abs/2107.02718">
                <papertitle>Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation</papertitle><br>
                </a>
                <b>Takehiko Ohkawa</b>, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato<br>
                <em><b>IEEE Access</b></em>, 2021<br>
                <a href="http://arxiv.org/abs/2107.02718">[Paper]</a>
                <a href="https://ieeexplore.ieee.org/document/9469781">[IEEE Xplore]</a>
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <a href="projects/21-fgsty-cpl/index.html">[Project]</a>
                <a href="https://github.com/ut-vision/FgSty-CPL">[Code & Data]</a>
                <label class="open" for="pop-up-2"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-2" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-2">×</label>
                    <div class="bibtex" id="bibtex-21-fgsty-cpl"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/21-fgsty-cpl/bibtex.txt';
                    load_text(filename, 'bibtex-21-fgsty-cpl');
                    </script>
                </div>
                </div>
                <p>
                We developed a domain adaptation method for hand segmentation, consisting of appearance gap reduction by stylization and learning with pseudo-labels generated by network consensus.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src="https://user-images.githubusercontent.com/28190044/106457189-f2d63880-64d1-11eb-9cfa-cc22e79b7809.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2003.00187">
                <papertitle>Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image Translation</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Naoto Inoue, Hirokatsu Kataoka, and Nakamasa Inoue
                <br>
                <em>International Conference on Pattern Recognition (<b>ICPR</b>)</em>, 2020
                <br>
                <!-- <a href="xxxxx">[Project]</a> / -->
                <a href="https://arxiv.org/abs/2003.00187">[Paper]</a>
                <label class="open" for="pop-up-1"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-1" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-1">×</label>
                    <div class="bibtex" id="bibtex-20-accr"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/20-accr/bibtex.txt';
                    load_text(filename, 'bibtex-20-accr');
                    </script>
                </div>
                </div>
                <p>
                We developed extended consistency regularization for stabilizing the training of image translation models using real, fake, and reconstructed samples.
                </p>
            </td></tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px 20px 10px 20px;width:100%;vertical-align:middle">
                <heading>Research & Work Experience</heading>
                <p>
                    <b>[Oct 2025 - Present]</b> Project Researcher, Institute of Industrial Science, UTokyo<br>
                    <b>[Nov 2020 - Sep 2025]</b> Research assistant, <a href="https://www.ut-vision.org/sato-lab/">Sato Lab</a>, UTokyo<br>
                    <b>[Sep 2024 - Aug 2025]</b> Research mentoring, <a href="https://deepmind.google/">Google DeepMind</a> @Tokyo<br>
                    <b>[Jun 2024 - Nov 2024]</b> Research internship, <a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a> @Pittsburgh<br>
                    <b>[Jul 2023 - Mar 2024]</b> Visiting researcher, <a href="https://cvg.ethz.ch/">CVG Group</a>, ETH Zurich<br>
                    <b>[Apr 2023 - Mar 2024]</b> Research collaboration, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> @Beijing<br>
                    <b>[Jan 2023 - May 2023]</b> Research internship, <a href="https://www.omron.com/sinicx/">OMRON SINIC X Corp.</a><br>
                    <b>[May 2022 - Nov 2022]</b> Research internship, <a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a> @Redmond<br>
                    <b>[Sep 2021 - Mar 2022]</b> Research scholar, <a href="http://www.cs.cmu.edu/~kkitani/">Kitani Lab</a>, CMU<br>             
                </p>
                </td>
            </tr>
            </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px 20px 10px 20px;width:100%;vertical-align:middle">
                <heading>Honors & Awards</heading>
                <p>
                    EgoVis Distinguished Paper Award@CVPR, 2025<br>
                    Google PhD Fellowship in Machine Perception, 2024<br>           
                    JSPS DC1 Special Stipends for Excellent Research Results, 2024<br>
                    JSPS Research Fellowship for Young Scientists (DC1), 2022-2024<br>
                    ETH Zurich Leading House Asia "Young Researchers' Exchange Programme", 2023<br>
                    Microsoft Research Asia Collaborative Research Program D-CORE, 2023<br>
                    UTokyo-IIS Research Collaboration Initiative Award, 2021<br>
                    MIRU Student Encouragement Award, 2021<br>
                    PRMU Best Presentation of the Month, 2020<br>
                </p>                
                </td>
            </tr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px 20px 10px 20px;width:100%;vertical-align:middle">
                <heading>Grants</heading>
                <p>                               
                    ACT-X Travel Grant for International Research Meetings, 2024<br>
                    UTokyo-IIS Travel Grant for International Research Meetings, 2024<br>
                    JST ACT-X Acceleration Phase of "Frontier of Mathematics and Information Science", 2023<br>
                    JST ACT-X "Frontier of Mathematics and Information Science", 2020-2022<br>
                    UTokyo-IIS Travel Grant for International Research Meetings, 2022<br>
                    JASSO Scholarship for Excellent Master Students at UTokyo, 2021<br>
                    JEES/Softbank AI Scholarship, 2020<br>
                    Tokio Marine Kagami Memorial Foundation Scholarship, 2018-2020<br>
                </p>                
                </td>
            </tr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px 20px 10px 20px;width:100%;vertical-align:middle">
                <heading>Talks</heading>
                <p>                
                    <!-- <span class="text-accent">[Upcoming!]</span>  -->
                    Google Developer Groups AI for Science - Japan, Dec 2025. <a href="https://gdg.community.dev/events/details/google-gdg-ai-for-science-japan-presents-shi-jue-karashen-ti-xing-wochi-tsuaihe-qiao-zhi-nadong-zuo-no3ci-yuan-li-jie/">[Link (ja)]</a><a href="https://speakerdeck.com/tkhkaeio/shi-jue-karashen-ti-xing-wochi-tuaihe-qiao-zhi-nadong-zuo-no3ci-yuan-li-jie">[Slides]</a><br>
                    IPSJ Seminar Series: "Frontiers in Sensing and Analyzing Human Behavior", Nov 2025. <a href="https://www.ipsj.or.jp/event/seminar/2025/program10.html">[Link (ja)]</a><br>
                    UIUC Vision Seminar (hosted by <a href="https://saurabhg.web.illinois.edu/">Saurabh Gupta</a>), "Understanding human hands in interactions”, Apr 2025.<br>
                    NUS Seminar (hosted by <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>), "Perceiving hand and action across ego-exo views", Jul 2023.<br>
                </p>
                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:10px 20px 10px 20px;width:100%;vertical-align:middle">
        <heading>Academic Activities</heading>
            <p>
            <b>Professional Service:</b> 
            <ul>
                <li>Reviewers: CVPR, ECCV, ICCV, ICLR, TPAMI, ACM MM, ACM IMWUT</li>
            </ul>
            </p>
            <p>
            <b>Organization:</b>
            <ul>
                <li>HANDS Workshop at ICCV 2025, ECCV 2024, and ICCV 2023</li>
            </ul>
            </p>
            <p>
            <b>Conference Travel History:</b>
            <ul>
                <li>2025: WACV (Tucson, USA), ICLR (Singapore), CVPR (Nashville, USA), ICCV (Hawaii, USA)</li>
                <li>2024: CVPR (Seattle, USA), ECCV (Milan, Italy)</li>
                <li>2023: CVPR (Vancouver, Canada), ICVSS (Sicily, Italy), ICCV (Paris, France)</li>
                <li>2022: ECCV (Tel-Aviv, Israel), SIGGRAPH (Vancouver, Canada), WACV (Hawaii, USA)</li>
                <li>2019: ICCV (Soul, Korea)</li>
            </ul>
            </p>
        </td>
        </tr>
    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:0px">
                <br>
                <p
                    style="text-align:center;font-size:small;">
                    <script type="text/javascript"
                    id="clustrmaps"
                    src="//cdn.clustrmaps.com/map_v2.js?cl=c9c1c1&w=300&t=n&d=j0j_3rEh4HCvS5DIxlRJ3P9J8yp0mtUeDLZ6Z6I2NJ8&co=ffffff&cmo=ea8d7a&cmn=1772d0&ct=000000"></script>
                </p>
                <p
                    style="text-align:right;font-size:small;">
                    © Takehiko Ohkawa /
                    Design: <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>
                </p>
                </td>
            </tr>
            </tbody></table>
        <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-162031573-1', 'auto');
        ga('send', 'pageview');
    </script>
        </td>
    </tr>
    </tbody></table><iframe frameborder="0" scrolling="no"
    style="border: 0px; display: none; background-color: transparent;"></iframe>
<div id="GOOGLE_INPUT_CHEXT_FLAG" input="null"
    input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}"
    style="display: none;"></div>
</body>

</html>
