<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Take Ohkawa</title>
  
  <meta name="author" content="Take Ohkawa">
  <meta name="description" content="Take Ohkawa's Homepage">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="googlef77166d9f74b3a8e.html">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="XXX"> -->
</head>

<body>
  <table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Take Ohkawa</name>
              </p>
              <p>
                Hi, I am <b>Take Ohkawa (大川 武彦)</b>.<br> I am a first-year Ph.D. student (2021-) at Graduate School of Information Science and Technology, The University of Tokyo, advised by Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a>.<br>
                
                During my Ph.D. journey, I am working at <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a> in 2022.
                I am also collaborating with Dr. <a href="http://yoshitakaushiku.net/">Yoshitaka Ushiku</a> at <a href="https://www.omron.com/sinicx/">OMRON SINIC X</a> and Prof. <a href="http://www.lsta.media.kyoto-u.ac.jp/home-e.html">Shinsuke Mori</a> at Kyoto University.
                I was fortunate to work under Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> at Carnegie Mellon University as a Research Scholar in 2021.
            </p>
            <p>
                I am the youngest Principal Investigator of <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X Project on "Human Behavior Understanding via Imitative AI Agents"</a> (2020-2023). I am <a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-22J20071/"> Research Fellow (DC1) of JSPS Research Fellowships for Young Scientists</a> (2022-2025).<br>
            </p>
              <p>
                I received my master's degree for one-and-a-half-year early graduation at The University of Tokyo under the supervision of Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a> and Prof. <a href="https://lab.rekimoto.org/members/rekimoto/">Jun Rekimoto</a>. Before that, I received my bachelor's degree for three-year early graduation at the Tokyo Institute of Technology under the guidance of Prof. <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Nakamasa Inoue</a>.
                <!-- <br> Prior to the bachelor's laboratory assignment, I conducted various research projects with Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ks.c.titech.ac.jp%2F~shinoda%2Findex.html&sa=D&sntz=1&usg=AFQjCNHYXCsAe_UkdMMRg7SxTCzSsS7-Cg">Koichi Shinoda</a>                            (Speech), Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ymatsuo.com%2F&sa=D&sntz=1&usg=AFQjCNGTFtL0e4HnNWViMbXcqxkygBoz5g">Yutaka Matsuo</a> (AI), Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.chokkan.org%2Findex.en.html&sa=D&sntz=1&usg=AFQjCNERO4X90fx_xrtsQEGLiczXnaWopA">Naoaki Okazaki</a>                            (NLP), and Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.cbi.c.titech.ac.jp%2Fsekijima.html&sa=D&sntz=1&usg=AFQjCNFSCKmGj9FkReAzjJiWYN2qzV5NIA">Masakazu Sekijima</a> (BioInfo). -->
                <br>
              </p>
              <p style="text-align:left">
                E-mail: ohkawa-t [at] iis.u-tokyo.ac.jp<br>
                <!-- <a href="mailto:xxx@x.com">Email</a> &nbsp/&nbsp -->
                <!-- <a href="assets/CV_2021_11.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="xxx.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=WNIxd2UAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/tkhkaeio">Twitter</a>
                <!-- &nbsp/&nbsp -->
                <!-- <a href="https://github.com/xxxx/">Github</a> -->
              </p>
            </td>
            <!-- Profile image -->
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:85%;max-width:85%;min-width:85%" alt="profile photo" src="https://user-images.githubusercontent.com/28190044/151274386-f3e0023d-0a8f-464c-b2fd-5dd9fd1d2fc6.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <p>
                    <b>[May 2022]</b> Started an internship at <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a>, Redmond.<br>
                    <b>[Feb 2022]</b> Received UTokyo-IIS Research Collaboration Initiative Award with <a href="https://twitter.com/tkhkaeio/status/1507297146772799491?s=20&t=0IM8HYvhrWsCrqv9SnAJzw">Oculus Quest</a>!<br>
                    <b>[Sep 2021]</b> Started working as a Research Scholar at <a href="http://www.cs.cmu.edu/~kkitani/">Kris Lab</a> at Robotics Institute, CMU.<br>
                    <b>[Sep 2021]</b> Obtained M.A.S. for 1.5 years (<a>Early Graduation</a>) at UTokyo.<br>
                    <b>[Jun 2021]</b> Paper <a href="http://arxiv.org/abs/2107.02718" border="0">"Domain Adaptation of Hand Segmentation"</a> got accepted to IEEE Access 2021.<br>
                    <details>
                    <summary>Past updates</summary>
                        <b>[Oct 2020]</b> My research proposal got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X</a>.<br>
                        <b>[Oct 2020]</b> Paper <a href="https://arxiv.org/abs/2003.00187" border="0">"Augmented Cyclic Consistency Regularization"</a> got accepted to ICPR2020.<br>
                        <b>[Aug 2020]</b> Started an internship at <a href="https://www.omron.com/sinicx/">OSX</a>, Tokyo.<br>
                        <!-- <b>[Jun 2020]</b> Prof. <a href="https://lab.rekimoto.org/members/rekimoto/">Jun Rekimoto</a> became my co-advisor.<br> -->
                        <b>[Apr 2020]</b> Joined <a href="https://www.ut-vision.org/sato-lab/">Sato/Sugano Lab</a> at UTokyo.<br>
                        <b>[Mar 2020]</b> Obtained B.E. for 3 years (<a>Early Graduation</a>) at TokyoTech.<br>
                        <b>[Oct 2019]</b> Gifted NVIDIA RTX 2080Ti from <a href="https://twitter.com/tkhkaeio/status/1186167767415672833?s=20">Yu Darvish</a>, a Japanese MLB player who I respect the most!<br>
                        <b>[Oct 2019]</b> Joined <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Inoue Lab</a> at TokyoTech.<br>
                    </details>
                </p>
    
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <blockquote style="font-style: italic;">
                    <p>"It is not the strongest of the species that survives, nor the most intelligent. It is the one most <b>adaptable to change</b>." <div style="text-align: right">-Charles Darwin (1809-1882)</div></p>
                </blockquote>
                C. Darwin left us with an astute analysis of how intelligence could function in the wild, which represents the fruit of his lifelong research.
                This analysis on the nature of intelligence inspires my research on machine intelligence.
                <br>
                
                <br>
                In my quest for robust, flexible, and reliable machine intelligence for understanding the visual world, I am engaging in the research of <b>computer vision</b> and <b>machine learning</b>. 
                My research objective is to <b>build a real-world visual perception system for understanding human behavior in diverse domains and applications</b>. I focus on extending recognition models for hand-object interaction and human activity to diverse deployment scenarios while alleviating dataset bias between training and test data.
                Specifically, I am working on <b>machine learning under limited labels and/or distribution shift</b> and <b>human behavior understanding in videos</b>, including domain adaptation, self-supervised (unsupervised) learning, hand-object interaction understanding, and first-person/embodied vision.
                
                <br><br><a href="publications.html" target="_blank">Full list of my publications</a> is available.
            </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        
        <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src="https://user-images.githubusercontent.com/28190044/172274533-aaaf7014-a379-4025-a6e2-ad9e96da7b75.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.02257">
                <papertitle>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em>arXiv Preprint</em>, 2022  
                <br>
                <a href="https://arxiv.org/abs/2206.02257">[Paper]</a>
                <a href="./projects/22-hpe-survey/bibtex.txt">[BibTex]</a>
                <p></p>
                <p>
                    We present comprehensive analysis of 3D hand pose estimation from the perspective of efficient annotation and learning, covering recent approaches for 3D hand pose annotation and learning methods with limited annotated data.
                </p>
            </td>
        </tbody>
        <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src="https://user-images.githubusercontent.com/28190044/159080951-fe11090c-664c-4ce1-bf11-67ae2184b7f9.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2203.08344">
                <papertitle>Domain Adaptive Hand Keypoint and Pixel Localization in the Wild</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, and Yoichi Sato
                <br>
                <em>arXiv Preprint</em>, 2022  
                <br>
                <a href="https://arxiv.org/abs/2203.08344">[Paper]</a>
                <a href="./projects/22-hand-ps-da/">[Project]</a>
                <a href="./projects/22-hand-ps-da/bibtex.txt">[BibTex]</a>
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <p></p>
                <p>
                    We tackled on domain adaptation of hand keypoint regression and hand segmentation to in-the-wild data with new imaging conditions (e.g., Ego4D).
                </p>
            </td>
        </tbody>
        <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src="https://user-images.githubusercontent.com/28190044/156375706-66ba3fe7-25f5-4405-a987-e787b993b362.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2202.13941">
                <papertitle>Background Mixup Data Augmentation for Hand and Object-in-Contact Detection</papertitle>
                </a>
                <br>
                Koya Tango, <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em>arXiv Preprint</em>, 2022  
                <br>
                <!-- <a href="xxxxx">project page</a> / -->
                <a href="https://arxiv.org/abs/2202.13941">[Paper]</a>
                <!-- <a href="./projects/20-accr/bibtex.txt">[BibTex]</a> -->
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <p></p>
                <p>
                    We propose Background Mixup augmentation that leverages data-mixing regularization for hand-object detection while avoiding unintended effect produced by naive Mixup.
                </p>
            </td>
        </tbody>
        <tbody>
          <!-- <tr onmouseout="idName_stop()" onmouseover="idName_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='id_name'><video  width=100% height=100% muted autoplay loop>
                <source src="images/xxx.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='https://user-images.githubusercontent.com/28190044/106457189-f2d63880-64d1-11eb-9cfa-cc22e79b7809.png' width="160">
              </div>
              <script type="text/javascript">
                function idName_start() {
                  document.getElementById('id_name').style.opacity = "1";
                }
                function idName_stop() {
                  document.getElementById('id_name').style.opacity = "0";
                }
                idName_stop()
              </script>
            </td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src="https://user-images.githubusercontent.com/28190044/124936861-dcfd0e80-e041-11eb-98bb-64ae613aa74c.gif" width="180" height="180">
                    <!-- <img src="https://user-images.githubusercontent.com/28190044/123925072-851d3280-d9c5-11eb-907e-70dd17d0225d.jpg" width="180"> -->
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2107.02718">
                <papertitle>Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation</papertitle><br>
              </a>
              <b>Takehiko Ohkawa</b>, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato<br>
              <em><b>IEEE Access</b></em>, 2021<br>
              <a href="http://arxiv.org/abs/2107.02718">[Paper]</a>
              <a href="https://ieeexplore.ieee.org/document/9469781">[IEEE Xplore]</a>
              <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
              <a href="projects/21-fgsty-cpl/index.html">[Project]</a>
              <a href="https://github.com/ut-vision/FgSty-CPL"> [Data & Code]</a>
              <a href="./projects/21-fgsty-cpl/bibtex.txt">[BibTex]</a>

              <p>
                  We developed a domain adaptation method for hand segmentation, consisting of 
                  appearance gap reduction by stylization and learning with pseudo-labels generated by network consensus.
              </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src="https://user-images.githubusercontent.com/28190044/106457189-f2d63880-64d1-11eb-9cfa-cc22e79b7809.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2003.00187">
                <papertitle>Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image Translation</papertitle>
              </a>
              <br>
              <b>Takehiko Ohkawa</b>, Naoto Inoue, Hirokatsu Kataoka, and Nakamasa Inoue
              <br>
              <em>International Conference on Pattern Recognition (<b>ICPR</b>)</em>, 2020  
              <br>
              <!-- <a href="xxxxx">project page</a> / -->
              <a href="https://arxiv.org/abs/2003.00187">[Paper]</a>
              <a href="./projects/20-accr/bibtex.txt">[BibTex]</a>
              <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
              <p></p>
              <p>
                  We developed extended consistency regularization for stabilizing the training of image translation models using real, fake, and reconstructed samples.
              </p>
            </td></tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research & Work Experience</heading>
              <p>
                <b>[Nov 2020 - Present]</b> Research assistant, <a href="https://www.ut-vision.org/sato-lab/">Sato Lab</a>, The University of Tokyo<br>
                <b>[May 2022 - Present]</b> Research internship, <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a> (Redmond, WA).<br>
                <b>[Feb 2019 - Present]</b> Student researcher, <a href="http://xpaperchallenge.org/cv/">cvpaper.challenge</a>, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.aist.go.jp%2Findex_en.html&sa=D&sntz=1&usg=AFQjCNHcFF2d0S_ymFehZa5cee7xxsDzww">AIST</a> <br>
                <b>[Sep 2021 - Mar 2022]</b> Research scholar, <a href="http://www.cs.cmu.edu/~kkitani/">Kris Lab</a>, Robotics Institute, Carnegie Mellon University<br>  
                <b>[Aug 2020 - Aug 2021]</b> Research internship, <a href="https://www.omron.com/sinicx/">OMRON SINIC X Corp.</a><br>
                <b>[Oct 2019 - May 2020]</b> Research internship, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.neuralpocket.com%2Fen%2F&sa=D&sntz=1&usg=AFQjCNHPYi2Zyj7HCozNbgGoqOxrXLViow">Neural Pocket Inc.</a><br>
                <b>[Aug 2019 - Mar 2020]</b> Research assistant, <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Inoue Lab</a>, Tokyo Institute of Technology<br>
                <b>[Aug 2019 - Sep 2019]</b> Engineering internship, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.teamlab.art%2F&sa=D&sntz=1&usg=AFQjCNGtI-cRUBi2GvUXaa0A-WPIxxyA6w">teamLab Inc.</a> <br>
                <b>[Dec 2017 - Nov 2018]</b> Research internship, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.cross-compass.com%2Fen%2Ffront-page%2F&sa=D&sntz=1&usg=AFQjCNHBhiK40Hg3ganunGUZLZIzIrUoXg"> Cross Compass Ltd. </a>
              </p>
            </td>
            </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards & Grants</heading>
              <p>
                JSPS Research Fellowship for Young Scientists (DC1), 2022-2025 (expected)<br>
                ACT-X "Frontier of Mathematics and Information Science", Japan Science and Technology Agency, 2020-2023 (expected)<br>
                UTokyo-IIS Research Collaboration Initiative Award, 2021<br>
                MIRU Student Encouragement Award, 2021<br>
                PRMU Best Presentation of the Month, 2020<br>
                JEES/Softbank AI Scholarship, 2020<br>
                Tokio Marine Kagami Memorial Foundation Scholarship, 2018-2020<br>
              </p>
            </td>
            </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Services & Activities</heading>
              <p>
                Visiting Research: <br>
                Reviewer: <br>
                &emsp;- European Conference on Computer Vision: ECCV2022<br>
                &emsp;- Meeting on Image Recognition and Understanding: MIRU2022
                <br>
                Invited Talks: <br>
              </p>
            </td>
            </tr>
        </tbody></table> -->
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=c9c1c1&w=150&t=n&d=j0j_3rEh4HCvS5DIxlRJ3P9J8yp0mtUeDLZ6Z6I2NJ8&co=ffffff&cmo=ea8d7a&cmn=1772d0&ct=000000'></script>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
            <br>
              <p style="text-align:right;font-size:small;">
                © Takehiko Ohkawa 2021 /
                Design: <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
        <script>
            (function(i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function() {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

            ga('create', 'UA-162031573-1', 'auto');
            ga('send', 'pageview');
        </script>
      </td>
    </tr>
  </tbody></table><iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe>
  <div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div>
</body>

</html>
