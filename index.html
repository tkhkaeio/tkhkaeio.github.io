<!DOCTYPE html>
<html lang="en">
<head>
<title>Take Ohkawa</title>
<meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<meta name="author" content="Take Ohkawa">
<meta name="description" content="Take Ohkawa's Homepage">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!-- <meta name="google-site-verification" content="googlef77166d9f74b3a8e.html"> -->
<meta name="google-site-verification" content="lrotvHHzOzWJnzATjx0FhDknYrDysUQuDse7oJyruA4" />
<link rel="stylesheet" type="text/css" href="stylesheet.css">
<!-- <link rel="icon" type="image/png" href="XXX"> -->
<script type="text/javascript">
        function load_text(filename, id){
            window.addEventListener('DOMContentLoaded', function(){
                fetch(filename)
                    .then(response => response.text())
                    .then(data => {
                    const file_area = document.getElementById(id);
                    file_area.innerHTML = data.replace(/\n/g, "<br>");
                    //file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&emsp;");
                    file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&nbsp;");
                });
            });
        };
</script>
</head>

<body>
<table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Take Ohkawa</name>
                </p>
                <p>
                    Hi, I am <b>Take Ohkawa (大川 武彦)</b>.<br>
                    I am a PhD student (2021-) at Graduate School of Information Science and Technology, The University of Tokyo, advised by
                    Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a>.<br><br>                        

                    During my PhD journey, I joined Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a> lab at ETH Zurich as a Visiting Researcher in 2023, and Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> lab at CMU as a Research Scholar in 2021. 
                    <br>
                    I interned at Meta Reality Labs to work with Dr. <a href="https://sites.google.com/view/takaaki-shiratori/home">Takaaki Shiratori</a> and Dr. <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a> in Pittsburgh in 2024, and with Dr. <a href="https://kunhe.github.io/">Kun He</a>,  Dr. <a href="https://fadimesener.github.io/">Fadime Sener</a>, and Dr. <a href="https://thodan.github.io/">Tomas Hodan</a> in Redmond in 2022.
                    I worked closely with Dr. <a href="http://yoshitakaushiku.net/">Yoshitaka Ushiku</a> and Dr. <a href="https://atsushihashimoto.github.io/cv/">Atsushi Hashimoto</a> at OMRON SINIC X.
                    <br><br>
                    Fortunately, I was awarded competitive fellowships from Google PhD Fellowship (2024), Microsoft Research Asia (2023), and ETH Zurich Leading House Asia (2023).
                    I also received EgoVis Distinguished Paper Award at CVPR 2025.
                    I served as Principal Investigator of JST ACT-X Project (2020-2023) and JSPS Research Fellow (DC1) (2022-2024).
                    <br>
                </p>
                <!-- <p>
                    I attained my master's degree with early completion in 1.5 years at The University of Tokyo under the supervision of Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a> and Prof. <a href="https://lab.rekimoto.org/members/rekimoto/">Jun Rekimoto</a>. 
                    Prior to that, I received my bachelor's degree with early completion in 3 years at the Tokyo Institute of Technology under the guidance of Prof. <a href="https://mmai.tech/">Nakamasa Inoue</a>.
                    <br>
                </p> -->
                <p style="text-align:left">
                    E-mail: ohkawa-t [at] iis.u-tokyo.ac.jp<br>
                    <!-- <a href="mailto:xxx@x.com">Email</a> &nbsp/&nbsp -->                    
                    <!-- <a href="xxx.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?hl=en&user=WNIxd2UAAAAJ&hl">Google
                    Scholar</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/tkhkaeio/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/tkhkaeio">X (Twitter)</a> &nbsp/&nbsp
                    <a href="https://drive.google.com/file/d/1xERt3NKcF-Zj68Sv-4CgkC7ai_uSYnLl/view?usp=sharing">CV</a>
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://github.com/xxxx/">Github</a> -->
                </p>
                <p>
                    <font color="#ff0000">I'm actively looking for research positions in academia or industry after graduation.
                        Please feel free to contact me if you are interested in my research.
                    </font>                   
                </p>
                </td>
                <!-- Profile image -->
                <td style="padding:2.5%;width:35%;max-width:35%;">                  
                    <img
                    style="width:100%;max-width:100%;min-width:100%"
                    alt="profile photo"
                    src="https://user-images.githubusercontent.com/28190044/270403272-9c544d58-e70a-40ca-a546-d233e450a350.jpeg"
                    class="hoverZoomLink">         
                    <!-- <figcaption>Gornergrat, Zermatt, Switzerland</figcaption> -->
                </td>                
            </tr>
            </tbody>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <!-- utokyo -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/77ab1572-37b7-4991-8e08-d71d3cea1717" width="120">
                <figcaption>PhD'25-'21</figcaption>
                </td>         
                <!-- meta -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/user-attachments/assets/802d5497-acf1-42fb-ae47-a177a2ff21ec" width="110">
                <figcaption>Intern'24,'22</figcaption>
                </td>            
                <!-- google -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/user-attachments/assets/4936c7db-f091-4bdd-b22d-4f05fb801773" width="95">
                <figcaption>Fellowship'24</figcaption>
                </td>
                <!-- eth zurich -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/6968d9e1-36c3-4f44-8683-f53485001620" width="120">
                <figcaption>Visitor'23</figcaption>
                </td>
                <!-- cmu ri -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/39f4ce4f-964e-4e25-b6d1-ee81cfb7a0fc" width="95">
                <figcaption>Visitor'21</figcaption>
                </td>                
                <!-- miscrosoft -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/15eb8318-fdb5-4b0b-b311-fb7b706e693f" width="140">
                <figcaption>Fellowship'23</figcaption>
                </td>                
                <!-- omron -->
                <td width="10%" valign="middle" align="center">
                <img src="https://gist.github.com/assets/28190044/286ce042-5a8b-4344-a5ee-e5d32451d311" width="100">
                <figcaption>Intern'23,'20</figcaption>
                </td>                     
            </tr>
            </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p> 
                    <b>[Jun 2025]</b> Paper <a href="https://arxiv.org/abs/tbd">"SCGen"</a> got accepted to ICCV 2025!<br>
                    <b>[Jun 2025]</b> Received <a href="https://egovis.github.io/awards/2023_2024/">EgoVis Distinguished Paper Award</a>, CVPR 2025!<br>
                    <b>[Apr 2025]</b> 9th <a href="https://hands-workshop.org/">HANDS</a> workshop proposal got aceepted to ICCV 2025! See you in Hawaii, US!<br>
                    <b>[Apr 2025]</b> Invited to give a talk at <a href="https://vision.cs.illinois.edu/vision_website/#five">UIUC Vision Seminar</a>!<br>
                    <b>[Jan 2025]</b> Paper <a href="https://arxiv.org/abs/2502.15251">"SiMHand"</a> got accepted to ICLR 2025.<br>
                    <b>[Nov 2024]</b> Received <a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2024">Google PhD Fellowship</a> in Machine Perception!<br>
                    <b>[Oct 2024]</b> Paper <a href="https://arxiv.org/abs/2311.16444">"Exo2EgoDVC"</a> got accepted to WACV 2025.<br>
                    <b>[Jul 2024]</b> Three papers <a href="https://arxiv.org/abs/2403.16428">"HANDS'23 Analysis"</a>, <a href="https://arxiv.org/abs/2311.17366">"GHTT"</a>, <a href="https://arxiv.org/abs/2409.09714">"HandCLR"</a> got accepted to ECCV 2024.<br>                             
                    <details>                        
                    <summary>Past updates</summary>
                    <b>[Jun 2024]</b> Started an internship at <a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a> @Pittsburgh.<br>       
                    <b>[Apr 2024]</b> Two papers <a href="https://arxiv.org/abs/2403.04381">"S2DHand"</a> and <a href="https://arxiv.org/abs/2311.16444">"Exo2EgoDVC"</a> got accepted to CVPR 2024.<br>
                    <b>[Apr 2024]</b> 8th <a href="https://hands-workshop.org/">HANDS</a> workshop proposal got aceepted to ECCV 2024 with Dr. <a href="https://www.mu4yang.com/">Linlin</a> at CUC. See you in Milan!<br>
                    <b>[Apr 2024]</b> Gave an invited presenatation at <a href="https://sites.google.com/ut-vision.org/aspire-hcvm/">JST ASPIRE HCVM</a> workshop, UTokyo-IIS.<br>
                    <b>[Jul 2023]</b> Paper <a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">"Survey on 3D Hand Pose Estimation"</a> got accepted to IJCV.<br>
                    <b>[Jul 2023]</b> Started working as a Visiting Researcher at <a href="https://cvg.ethz.ch/">CVG Group</a>, ETH Zurich.<br>    
                    <b>[Jun 2023]</b> Invited to give a talk at <a href="https://cvml.comp.nus.edu.sg/">CVML Group</a>, NUS.<br>
                    <b>[Apr 2023]</b> Research proposals got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X Acceleration Phase</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>.<br>
                    <b>[Mar 2023]</b> Host 7th <a href="https://sites.google.com/view/hands2023/">HANDS</a> workshop at ICCV 2023 with Prof. <a href="https://www.comp.nus.edu.sg/~ayao/">Angela</a> at NUS. See you in Paris!<br>
                    <b>[Feb 2023]</b> Paper <a href="https://assemblyhands.github.io/">"AssemblyHands"</a> got accepted to CVPR 2023.<br>
                    <b>[Jul 2022]</b> Paper <a href="https://arxiv.org/abs/2203.08344">"Hand State Estimation in the Wild"</a> got accepted to ECCV 2022.<br>       
                    <b>[Feb 2022]</b> Received UTokyo-IIS Research Collaboration Initiative Award with <a href="https://twitter.com/tkhkaeio/status/1507297146772799491?s=20&t=0IM8HYvhrWsCrqv9SnAJzw">Oculus Quests</a>!<br>        
                    <b>[Sep 2021]</b> Obtained M.A.S. for 1.5 years (<a>Early Graduation</a>), UTokyo.<br>
                    <b>[Jun 2021]</b> Paper <a href="http://arxiv.org/abs/2107.02718">"Domain Adaptation of Hand Segmentation"</a> got accepted to IEEE Access 2021.<br>
                    <b>[Oct 2020]</b> Research proposal got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X</a>.<br>
                    <b>[Oct 2020]</b> Paper <a href="https://arxiv.org/abs/2003.00187">"Augmented Cyclic Consistency Regularization"</a> got accepted to ICPR2020.<br>
                    <b>[Apr 2020]</b> Joined <a href="https://www.ut-vision.org/sato-lab/">Sato/Sugano Lab</a> at UTokyo.<br>
                    <b>[Mar 2020]</b> Obtained B.E. for 3 years (<a>Early Graduation</a>) at TokyoTech.<br>
                    <b>[Oct 2019]</b> Gifted NVIDIA RTX 2080Ti from <a href="https://twitter.com/tkhkaeio/status/1186167767415672833?s=20">Yu Darvish</a>, a Japanese MLB player who I respect the most!<br>
                    <b>[Oct 2019]</b> Joined <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Inoue Lab</a> at TokyoTech.<br>
                    </details>
                </p>

                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                    My research focuses on <b>computer vision for human sensing and understanding</b>, striving to achieve this from dual perspectives: precisely estimating external states of humans, such as physical poses, as well as inferring their internal states, such as intentions. 
                    This approach facilitates recognizing human interactions in the real world, connecting humans with the virtual world, and augmenting our perceptual capabilities via assistive AI systems.
                    Specifically, I am keenly interested in the following topics:
                    <ul>
                        <li>pose estimation and 3D reconstruction</li>
                        <li>video and activity understanding</li>
                        <li>egocentric vision and AR/VR technologies</li>
                        <li>self-supervised learning and transfer learning</li>
                        <li>human-computer intereaction</li>
                    </ul>
                </p>
                </td>
            </tr>
            </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                     <img src="projects/25-scgen/figs/scgen.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/tbd">
                    <papertitle>Generative Modeling of Shape-Dependent Self-Contact Human Poses</papertitle>
                    </a>
                    <br>
                    <b>Takehiko Ohkawa</b>, Jihyun Lee, Shunsuke Saito, Jason Saragih, Fabian Prado, Yichen Xu, Shoou-I Yu, Ryosuke Furuta, Yoichi Sato, and Takaaki Shiratori<br>
                    <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2025<br>
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/tbd">[Paper (TBD)]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                        We introduce the first extensive self-contact dataset with precise body shape registration, Goliath-SC, consisting of 383K self-contact poses across 130 subjects. Using this dataset, we propose generative modeling of a self-contact prior conditioned by body shape parameters, based on a body-part-wise latent diffusion with self-attention.
                    </p>
                </td>   
            <tbody>
                
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/278592ae-c87c-47a6-9793-b92d92a244ce"
                        width="180" height="auto">
                        <!-- class="img-fit"> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://hands-workshop.org/">
                    <papertitle>HANDS Workshops: Observing and Understanding Hands in Action</papertitle>
                    </a><br>                    
                    Contributed as an organizer and challenge committee<br>                    
                    <em>International Conference on Computer Vision Workshops (<b>ICCVW</b>)</em>, 2025<br>
                    Past editions: <a href="https://hands-workshop.org/workshop2024.html">[ECCV2024]</a> / <a href="https://sites.google.com/view/hands2023/">[ICCV2023]</a> <br>
                    <details>                             
                        <summary>Details</summary>                                                   
                        Hosted challenges:<br>
                        <a href="https://codalab.lisn.upsaclay.fr/competitions/19885#results">AssemblyHands-S2D</a>@ECCV2024: Dual-view hand pose<br>
                        <a href="https://codalab.lisn.upsaclay.fr/competitions/15149#results">AssemblyHands</a>@ICCV2023: Single-view hand pose<br>
                    </details>
                    <p></p>
                    <p>
                        Our HANDS workshop will gather vision researchers working on perceiving hands performing actions, including 2D & 3D hand detection, segmentation, pose/shape estimation, tracking, etc. We will also cover related applications including gesture recognition, hand-object manipulation analysis, hand activity understanding, and interactive interfaces.
                    </p>
                </td>
            </tbody>
            
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">                    
                    <!-- <img src="https://gist.github.com/user-attachments/assets/88610931-68ba-4af7-b319-778e54f55116" width="180" height="180"> -->
                     <img src="projects/25-emosign/figs/emosign.png" width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2505.17090">
                    <papertitle>EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language</papertitle>
                    </a>
                    <br>
                    Phoebe Chua*, Cathy Mengying Fang*, <b>Takehiko Ohkawa</b>, Raja Kushalnagar, Suranga Nanayakkara, and Pattie Maes (*equal contribution)
                    <br>
                    Preprint<br>
                    <!-- <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2024<br> -->
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2505.17090">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                        We introduce EmoSign, the first sign video dataset containing sentiment and emotion labels for 200 American Sign Language (ASL) videos with open-ended descriptions of emotion cues.
                        Alongside the annotations, we include baseline models for sentiment and emotion classification.
                        
                    </p>
                </td>   
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/user-attachments/assets/f6108781-2808-430e-8005-07e18ca7f4b9"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2502.15251">
                    <papertitle>SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training</papertitle>
                    </a>
                    <br>                    
                    Nie Lin*, <b>Takehiko Ohkawa*</b>, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, and Yoichi Sato (*equal contribution)
                    <br>
                    <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2025<br>
                    <a href="https://hands-workshop.org/workshop2024.html">HANDS</a>, <em>European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2024<br>                    
                    <a href="./projects/25-simhand/index.html">[Project]</a>
                    <a href="https://arxiv.org/abs/2502.15251">[Paper]</a>
                    <a href="https://github.com/ut-vision/SiMHand">[Code]</a> 
                    <a href="https://drive.google.com/file/d/1juP-FlV9o5pTYVi7mviTU4DMcKiAiovB/view?usp=sharing">[Poster]</a>
                    <!-- <a href="https://openreview.net/forum?id=xxx">[OpenReview]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <label class="open" for="pop-up-simhand"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-simhand" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-simhand">×</label>
                        <div class="bibtex" id="bibtex-25-simhand"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-simhand/bibtex.txt';
                        load_text(filename, 'bibtex-25-simhand');
                        </script>
                    </div>
                    </div>

                    <p></p>
                    <p>
                        We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SiMHand.
                    </p>
                </td>
            </tbody>
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/19d8b429-5ea1-4ad2-8aca-04e15fbc8609"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2311.16444">
                    <papertitle>Exo2EgoDVC: Dense Video Captioning of Egocentric Human Activities Using Web Instructional Videos</papertitle>
                    </a>
                    <br>
                    <b>Takehiko Ohkawa</b>, Takuma Yagi, Taichi Nishimura, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato
                    <br>
                    <em>IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>)</em>, 2025<br>
                    <a href="https://sites.google.com/view/lpvl-cvpr2024/home">LPVL</a>,                    
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (<b>CVPRW</b>)</em>, 2024<br>
                    <!-- <br> -->
                    <a href="./projects/25-egodvc/index.html">[Project]</a>
                    <a href="https://arxiv.org/abs/2311.16444">[Paper]</a>
                    <a href="https://github.com/ut-vision/Exo2EgoDVC">[Data & Code]</a>
                    <a href="https://youtu.be/rJvuLFrGBGQ">[Video]</a>
                    <label class="open" for="pop-up-7"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-7" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-7">×</label>
                        <div class="bibtex" id="bibtex-25-egodvc"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/25-egodvc/bibtex.txt';
                        load_text(filename, 'bibtex-25-egodvc');
                        </script>
                    </div>
                    </div>
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We present EgoYC2, a novel benchmark for cross-view knowledge transfer of dense video captioning, adapting models from web instructional videos with exocentric views to an egocentric view.
                    </p>
                </td>
            </tbody>                        
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/cb58ae2c-68e1-4766-95fd-401e13923634"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2311.17366">
                    <papertitle>Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling</papertitle>
                    </a>
                    <br>
                    Yilin Wen, Hao Pan, <b>Takehiko Ohkawa</b>, Lei Yang, Jia Pan, Yoichi Sato, Taku Komura, and Wenping Wang
                    <br>
                    <a href="https://hands-workshop.org/workshop2024.html">HANDS</a>, <em>European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2024<br>                    
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2311.17366">[Paper]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We present a novel framework that concurrently tackles hand action recognition and 3D future hand motion prediction.
                    </p>
                </td>
            </tbody>
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/b3584bf6-1e28-46a7-9b27-27b45b889795"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2403.16428">
                    <papertitle>Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects</papertitle>
                    </a>
                    <br>
                    Zicong Fan*, <b>Takehiko Ohkawa*</b>, Linlin Yang*, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, and Angela Yao (*equal contribution)
                    <br>
                    <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2024<br>
                    <!-- <br> -->
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2403.16428">[Paper]</a>
                    <a href="https://drive.google.com/file/d/1eFyXGjFeB-aASpWaCFsZ7VOK1nQMAtM-/view?usp=drive_link">[Poster]</a>
                    <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We present a comprehensive summary of the HANDS23 challenge using the AssemblyHands and ARCTIC datasets. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. 
                    </p>
                </td>
            </tbody>            
            <tbody>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <img
                        src="https://gist.github.com/assets/28190044/40ae7198-1e27-4d12-a10c-dbc55e4e30b3"
                        width="180" height="180">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2403.04381">
                    <papertitle>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</papertitle>
                    </a>
                    <br>
                    Ruicong Liu, <b>Takehiko Ohkawa</b>, Mingfang Zhang, Yoichi Sato
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2024<br>                    
                    Invited Poster Presentation at <em><a href="https://egovis.github.io/cvpr24/">EgoVis Workshop</a>, <b>CVPRW</b></em>, 2024<br>
                    Invited Oral Presentation at <em>Forum on Information Technology (<b>FIT</b>)</em>, 2024<br>
                    <!-- <a href="xxxxx">[Project]</a> / -->
                    <a href="https://arxiv.org/abs/2403.04381">[Paper]</a>                    
                    <a href="https://github.com/ut-vision/S2DHand">[Code]</a>
                    <label class="open" for="pop-up-6"><a>[BibTex]</a></label>
                    <input type="checkbox" id="pop-up-6" style="display: none;">
                    <div class="overlay">
                    <div class="window">
                        <label class="close" for="pop-up-6">×</label>
                        <div class="bibtex" id="bibtex-24-s2dhand"></div>
                        <script>
                        var filename = 'https://tkhkaeio.github.io/projects/24-s2dhand/bibtex.txt';
                        load_text(filename, 'bibtex-24-s2dhand');
                        </script>
                    </div>
                    </div>

                    <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                    <p></p>
                    <p>
                    We propose a novel Single-to-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views.
                    </p>
                </td>
            </tbody>            
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <!-- <img src="https://user-images.githubusercontent.com/28190044/224896516-0c474c0b-a76a-4de0-b539-fc4a93b048d5.png" width="180" height="180"> -->
                <img src="https://user-images.githubusercontent.com/28190044/228722223-dc127074-61e5-4b37-92dc-058a73780706.gif" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2304.12301">
                <papertitle>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, and Cem Keskin
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023<br>
                <em><a href="https://egovis.github.io/awards/2023_2024/">EgoVis Distinguished Paper Award</a>, <b>CVPR</b></em>, 2025<br>
                Invited Oral Presentation at <em><a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/home?authuser=0">Ego4D & EPIC Workshop</a>, <b>CVPRW</b></em>, 2023<br>
                Poster Presentation at <em>International Computer Vision Summer School (<b>ICVSS</b>)</em>, 2023<br>
                <a href="https://sites.google.com/view/hands2023/challenges/assemblyhands">HANDS Workshop Benchmark Dataset</a>, <em><b>ICCVW</b></em>, 2023<br>
                <a href="https://arxiv.org/abs/2304.12301">[Paper]</a>
                <a href="https://assemblyhands.github.io/">[Project]</a>                
                <!-- <a href="https://codalab.lisn.upsaclay.fr/competitions/15149#results">[Leaderboard]</a> -->
                <a href="https://github.com/facebookresearch/assemblyhands-toolkit">[Code & Data]</a>                
                <label class="open" for="pop-up-5"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-5" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-5">×</label>
                    <div class="bibtex" id="bibtex-23-assemblyhands"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/23-assemblyhands/bibtex.txt';
                    load_text(filename, 'bibtex-23-assemblyhands');
                    </script>
                </div>
                </div>

                <p>
                We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of challenging hand-object interactions from egocentric videos.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/172274533-aaaf7014-a379-4025-a6e2-ad9e96da7b75.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">
                <papertitle>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em>International Journal of Computer Vision (<b>IJCV</b>)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2206.02257">[Paper]</a>
                <a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">[Springer]</a>
                <a href="https://drive.google.com/file/d/15gHEeyeCuzFyGYBMK971U_524M55iR1d/view?usp=share_link">[Slides]</a>
                <label class="open" for="pop-up-4"><a>[BibTex]</a></label>                
                <input type="checkbox" id="pop-up-4" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-4">×</label>
                    <div class="bibtex" id="bibtex-22-hpe-survey"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/22-hpe-survey/bibtex.txt';
                    load_text(filename, 'bibtex-22-hpe-survey');
                    </script>
                </div>
                </div>

                <p>
                We present a systematic review of 3D hand pose estimation from the perspective of efficient annotation and learning. 3D hand pose estimation has been an important research area owing to its potential to enable various applications, such as video understanding, AR/VR, and robotics. 
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/159080951-fe11090c-664c-4ce1-bf11-67ae2184b7f9.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2203.08344">
                <papertitle>Domain Adaptive Hand Keypoint and Pixel Localization in the Wild</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, and Yoichi Sato<br>
                <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022<br>
                Invited Poster Presentation at <em><a href="https://sites.google.com/view/hands2022/home">HANDS</a> and <a href="https://sites.google.com/view/egocentric-hand-body-activity/home?authuser=0">HBHA</a> workshops, <b>ECCVW</b></em>, 2022<br>
                Invited Oral Presentation at <em>Meeting on Image Recognition and Understanding (<b>MIRU</b>)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2203.08344">[Paper]</a>
                <a href="./projects/22-hand-ps-da/">[Project]</a>
                <a href="https://drive.google.com/file/d/1Ub_-653uSLYb70YeeXpE67H2MDWOrr1z/view?usp=share_link">[Slides]</a>
                <label class="open" for="pop-up-3"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-3" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-3">×</label>
                    <div class="bibtex" id="bibtex-22-hand-ps-da"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/22-hand-ps-da/bibtex.txt';
                    load_text(filename, 'bibtex-22-hand-ps-da');
                    </script>
                </div>
                </div>
                <p>
                We tackled domain adaptation of hand keypoint regression and hand segmentation to in-the-wild egocentric videos with new imaging conditions (e.g., Ego4D).
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/156375706-66ba3fe7-25f5-4405-a987-e787b993b362.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2202.13941">
                <papertitle>Background Mixup Data Augmentation for Hand and Object-in-Contact Detection</papertitle>
                </a>
                <br>
                Koya Tango, <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em><a href="https://sites.google.com/view/hands2022/home">HANDS</a>, European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2022

                <br>
                <!-- <a href="xxxxx">[Project]</a> / -->
                <a href="https://arxiv.org/abs/2202.13941">[Paper]</a>
                <!-- <a href="./projects/20-accr/xxx.txt">[BibTex]</a> -->
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <p></p>
                <p>
                We propose Background Mixup augmentation that leverages data-mixing regularization for hand-object detection while avoiding unintended effect produced by naive Mixup.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src="https://user-images.githubusercontent.com/28190044/124936861-dcfd0e80-e041-11eb-98bb-64ae613aa74c.gif" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://arxiv.org/abs/2107.02718">
                <papertitle>Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation</papertitle><br>
                </a>
                <b>Takehiko Ohkawa</b>, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato<br>
                <em><b>IEEE Access</b></em>, 2021<br>
                <a href="http://arxiv.org/abs/2107.02718">[Paper]</a>
                <a href="https://ieeexplore.ieee.org/document/9469781">[IEEE Xplore]</a>
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <a href="projects/21-fgsty-cpl/index.html">[Project]</a>
                <a href="https://github.com/ut-vision/FgSty-CPL">[Code & Data]</a>
                <label class="open" for="pop-up-2"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-2" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-2">×</label>
                    <div class="bibtex" id="bibtex-21-fgsty-cpl"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/21-fgsty-cpl/bibtex.txt';
                    load_text(filename, 'bibtex-21-fgsty-cpl');
                    </script>
                </div>
                </div>
                <p>
                We developed a domain adaptation method for hand segmentation, consisting of appearance gap reduction by stylization and learning with pseudo-labels generated by network consensus.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src="https://user-images.githubusercontent.com/28190044/106457189-f2d63880-64d1-11eb-9cfa-cc22e79b7809.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2003.00187">
                <papertitle>Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image Translation</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Naoto Inoue, Hirokatsu Kataoka, and Nakamasa Inoue
                <br>
                <em>International Conference on Pattern Recognition (<b>ICPR</b>)</em>, 2020
                <br>
                <!-- <a href="xxxxx">[Project]</a> / -->
                <a href="https://arxiv.org/abs/2003.00187">[Paper]</a>
                <label class="open" for="pop-up-1"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-1" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-1">×</label>
                    <div class="bibtex" id="bibtex-20-accr"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/20-accr/bibtex.txt';
                    load_text(filename, 'bibtex-20-accr');
                    </script>
                </div>
                </div>
                <p>
                We developed extended consistency regularization for stabilizing the training of image translation models using real, fake, and reconstructed samples.
                </p>
            </td></tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research & Work Experience</heading>
                <p>
                    <b>[Nov 2020 - Present]</b> Research assistant, <a href="https://www.ut-vision.org/sato-lab/">Sato Lab</a>, UTokyo<br>
                    <b>[Sep 2024 - Aug 2025]</b> Research mentoring, <a href="https://deepmind.google/">Google DeepMind</a> @Tokyo.<br>
                    <b>[Jun 2024 - Nov 2024]</b> Research internship, <a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a> @Pittsburgh<br>
                    <b>[Jul 2023 - Mar 2024]</b> Visiting researcher, <a href="https://cvg.ethz.ch/">CVG Group</a>, ETH Zurich<br>
                    <b>[Apr 2023 - Mar 2024]</b> Research collaboration, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> @Beijing<br>
                    <b>[Jan 2023 - May 2023]</b> Research internship, <a href="https://www.omron.com/sinicx/">OMRON SINIC X Corp.</a><br>
                    <b>[May 2022 - Nov 2022]</b> Research internship, <a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a> @Redmond<br>
                    <b>[Sep 2021 - Mar 2022]</b> Research scholar, <a href="http://www.cs.cmu.edu/~kkitani/">Kitani Lab</a>, CMU<br>
                    <!-- <b>[Aug 2020 - Aug 2021]</b> Research internship, <a href="https://www.omron.com/sinicx/">OMRON SINIC X Corp.</a><br>
                    <b>[Oct 2019 - May 2020]</b> Research internship, <a href="https://www.neural-group.com/en/index.html">Neural Pocket Inc.</a><br>
                    <b>[Aug 2019 - Mar 2020]</b> Research assistant, <a href="https://mmai.tech/">Inoue Lab</a>, TokyoTech<br>
                    <b>[Aug 2019 - Sep 2019]</b> Engineering internship, <a href="https://www.teamlab.art/">teamLab Inc.</a> <br>
                    <b>[Dec 2017 - Nov 2018]</b> Research internship, <a href="https://www.cross-compass.com/en">Cross Compass Ltd. </a> -->
                </p>
                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Awards & Fellowships</heading>
                <p>
                    <font color="#ff0000">[New!]</font> EgoVis Distinguished Paper Award@CVPR, 2025<br>
                    Google PhD Fellowship in Machine Perception, 2024<br>           
                    JSPS DC1 Special Stipends for Excellent Research Results, 2024<br>
                    JSPS Research Fellowship for Young Scientists (DC1), 2022-2024<br>
                    ETH Zurich Leading House Asia "Young Researchers' Exchange Programme", 2023<br>
                    Microsoft Research Asia Collaborative Research Program D-CORE, 2023<br>
                    UTokyo-IIS Research Collaboration Initiative Award, 2021<br>
                    MIRU Student Encouragement Award, 2021<br>
                    PRMU Best Presentation of the Month, 2020<br>
                </p>                
                </td>
            </tr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Grants</heading>
                <p>                               
                    ACT-X Travel Grant for International Research Meetings, 2024<br>
                    UTokyo-IIS Travel Grant for International Research Meetings, 2024<br>
                    JST ACT-X Acceleration Phase of "Frontier of Mathematics and Information Science", 2023<br>
                    JST ACT-X "Frontier of Mathematics and Information Science", 2020-2022<br>
                    UTokyo-IIS Travel Grant for International Research Meetings, 2022<br>
                    JASSO Scholarship for Excellent Master Students at UTokyo, 2021<br>
                    JEES/Softbank AI Scholarship, 2020<br>
                    Tokio Marine Kagami Memorial Foundation Scholarship, 2018-2020<br>
                </p>                
                </td>
            </tr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Talks</heading>
                <p>                
                    <font color="#ff0000">[Upcoming!]</font> IPSJ Seminar Series: "Frontiers in Sensing and Analyzing Human Behavior", Nov 2025. <a href="http://ipsj.or.jp/event/seminar/2025/program10.html">[Link (ja)]</a><br> 
                    <!-- (情報処理学会 連続セミナー:<a href="http://ipsj.or.jp/event/seminar/2025/program10.html">人間行動センシングと解析技術の最前線</a>)<br> -->
                    UIUC Vision Seminar (hosted by <a href="https://saurabhg.web.illinois.edu/">Saurabh Gupta</a>), "Understanding human hands in interactions”, Apr 2025.<br>
                    NUS Seminar (hosted by <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>), "Perceiving hand and action across ego-exo views", Jul 2023.<br>
                </p>
                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Activities</heading>
            <p>
            <b>Professional Service:</b> 
            <ul>
                <!-- Reviewers: CVPR'24-, ECCV'24-, ICCV'25-, ACMMM'25-<br>  -->
                Reviewers: CVPR, ECCV, ICCV, ACMMM<br> 
                Workshop organizers:  HANDS (ICCV'25, ECCV'24, ICCV'23)<br>
            </ul>
            </p>
            <!-- <p>
            <b>Lab Visits:</b>
            <ul>
                <li>2023: 
                    Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>@ETH,
                    Prof. <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>@NUS
                    </li>
                <li>2022: 
                        Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>@ETH,
                        Dr. <a href="https://de.linkedin.com/in/fadime-sener-674064156">Fadime Sener</a>, 
                        Dr. <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a> & Dr. <a href="https://btekin.github.io/">Buğra Tekin</a> @Meta (Zurich),
                        Dr. <a href="http://robotics.naist.jp/members/j-taka/">Jun Takamatsu</a>@Microsoft (Redmond), Dr. <a href="https://radmahdi.github.io/Home.html">Mahdi Rad</a>@Microsoft (Zurich).
                        </li>
                <li>2021: Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> @CMU</li>
                <li>2019: Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu">Jiwen Lu</a> @Tsinghua Univ., 
                        Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ks.c.titech.ac.jp%2F~shinoda%2Findex.html&sa=D&sntz=1&usg=AFQjCNHYXCsAe_UkdMMRg7SxTCzSsS7-Cg">Koichi Shinoda</a> @TokyoTech, 
                        Dr. <a href="http://hirokatsukataoka.net">Hirokatsu Kataoka</a> @AIST, 
                        Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.cbi.c.titech.ac.jp%2Fsekijima.html&sa=D&sntz=1&usg=AFQjCNFSCKmGj9FkReAzjJiWYN2qzV5NIA">Masakazu Sekijima</a>@TokyoTech</li>
                <li>2018: Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.chokkan.org%2Findex.en.html&sa=D&sntz=1&usg=AFQjCNERO4X90fx_xrtsQEGLiczXnaWopA">Naoaki Okazaki</a> @TokyoTech, 
                        Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ymatsuo.com%2F&sa=D&sntz=1&usg=AFQjCNGTFtL0e4HnNWViMbXcqxkygBoz5g">Yutaka Matsuo</a> @UTokyo</li>
            </ul>
            </p> -->
            <p>
            <b>Conference Log:</b>
            <ul>
                <li>2025: WACV (Tucson, USA), ICLR (Singapore), CVPR (Nashville, USA)</li>
                <li>2024: CVPR (Seattle, USA), ECCV (Milan, Italy)</li>
                <li>2023: CVPR (Vancouver, Canada), ICVSS (Sicily, Italy), ICCV (Paris, France)</li>
                <li>2022: ECCV (Tel-Aviv, Israel), SIGGRAPH (Vancouver, Canada), WACV (Hawaii, USA)</li>
                <li>2019: ICCV (Soul, Korea)</li>
            </ul>
        <!-- Invited Talks: <br> -->
            </p>
        </td>
        </tr>
    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:0px">
                <br>
                <p
                    style="text-align:center;font-size:small;">
                    <script type="text/javascript"
                    id="clustrmaps"
                    src="//cdn.clustrmaps.com/map_v2.js?cl=c9c1c1&w=300&t=n&d=j0j_3rEh4HCvS5DIxlRJ3P9J8yp0mtUeDLZ6Z6I2NJ8&co=ffffff&cmo=ea8d7a&cmn=1772d0&ct=000000"></script>
                </p>
                <p
                    style="text-align:right;font-size:small;">
                    © Takehiko Ohkawa /
                    Design: <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>
                </p>
                </td>
            </tr>
            </tbody></table>
        <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-162031573-1', 'auto');
        ga('send', 'pageview');
    </script>
        </td>
    </tr>
    </tbody></table><iframe frameborder="0" scrolling="no"
    style="border: 0px; display: none; background-color: transparent;"></iframe>
<div id="GOOGLE_INPUT_CHEXT_FLAG" input="null"
    input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}"
    style="display: none;"></div>
</body>

</html>
