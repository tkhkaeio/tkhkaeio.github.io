<!DOCTYPE HTML>
<html lang="en">
<head>
<title>Take Ohkawa</title>
<meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<meta name="author" content="Take Ohkawa">
<meta name="description" content="Take Ohkawa's Homepage">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="google-site-verification"
    content="googlef77166d9f74b3a8e.html">
<link rel="stylesheet" type="text/css" href="stylesheet.css">
<!-- <link rel="icon" type="image/png" href="XXX"> -->
<script type="text/javascript">
        function load_text(filename, id){
            window.addEventListener('DOMContentLoaded', function(){
                fetch(filename)
                    .then(response => response.text())
                    .then(data => {
                    const file_area = document.getElementById(id);
                    file_area.innerHTML = data.replace(/\n/g, "<br>");
                    //file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&emsp;");
                    file_area.innerHTML = file_area.innerHTML.replace(/\s+/g, "&nbsp;");
                });
            });
        };
</script>
</head>

<body>
<table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Take Ohkawa</name>
                </p>
                <p>
                    Hi, I am <b>Take Ohkawa (大川 武彦)</b>.<br>
                    I am a PhD student (2021-) at Graduate School of Information Science and Technology, The University of Tokyo, advised by
                    Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a>.<br><br>                        

                    During my PhD journey,
                    I did a lot of work with Dr. <a href="http://yoshitakaushiku.net/">Yoshitaka Ushiku</a> at OMRON SINIC X.
                    I joined Meta Reality Labs as an intern and worked closely with Dr. <a href="https://kunhe.github.io/">Kun He</a> in 2022.
                    I was fortunate to work under Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> at CMU as a Research Scholar in 2021.
                </p>
                <p>
                    I am a Principal Investigator of <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X Project</a> (2020-2024), and a Research Fellow (DC1) of <a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-22J20071/">JSPS Research Fellowships for Young Scientists</a> (2022-2025).
                    I am a recipient of the reseach grant from Microsoft Research Asia (2023).
                    <br>
                </p>
                <p>
                    I attained my master's degree with early completion in 1.5 years at The University of Tokyo under the supervision of Prof. <a href="https://sites.google.com/ut-vision.org/ysato/home?authuser=0">Yoichi Sato</a> and Prof. <a href="https://lab.rekimoto.org/members/rekimoto/">Jun Rekimoto</a>. 
                    Prior to that, I received my bachelor's degree with early completion in 3 years at the Tokyo Institute of Technology under the guidance of Prof. <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Nakamasa Inoue</a>.
                    <br>
                </p>
                <p style="text-align:left">
                    E-mail: ohkawa-t [at]iis.u-tokyo.ac.jp<br>
                    <!-- <a href="mailto:xxx@x.com">Email</a> &nbsp/&nbsp -->
                    <!-- <a href="assets/CV_2021_11.pdf">CV</a> &nbsp/&nbsp -->
                    <!-- <a href="xxx.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?hl=en&user=WNIxd2UAAAAJ&hl">Google
                    Scholar</a> &nbsp/&nbsp
                    <a href="https://twitter.com/tkhkaeio">Twitter</a>
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://github.com/xxxx/">Github</a> -->
                </p>
                </td>
                <!-- Profile image -->
                <td style="padding:2.5%;width:30%;max-width:30%">
                <img
                    style="width:85%;max-width:85%;min-width:85%"
                    alt="profile photo"
                    src="https://user-images.githubusercontent.com/28190044/151274386-f3e0023d-0a8f-464c-b2fd-5dd9fd1d2fc6.png"
                    class="hoverZoomLink">
                </td>
            </tr>              
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                    <b>[Apr 2023]</b> Research proposals got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X Acceleration Phase</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>.<br>
                    <!-- <b>[Mar 2023]</b> Workshop proposal on <a href="https://sites.google.com/view/hands2022/home?authuser=0">HANDS</a> got accepted to ICCV 2023. See you in Paris!<br> -->
                    <b>[Feb 2023]</b> Paper <a href="https://assemblyhands.github.io/">"AssemblyHands Benchmark"</a> got accepted to CVPR 2023.<br>
                    <b>[Dec 2022]</b> Presentation slides for <a href="https://drive.google.com/file/d/1cVn2Rlozsb0dW6rBe15XcvmG_oeixZgA/view?usp=share_link">"Survey on 3D Hand Pose Estimation"</a> was available.<br>
                    <b>[Jul 2022]</b> Paper <a href="https://arxiv.org/abs/2203.08344">"Hand State Estimation in the Wild"</a> got accepted to ECCV 2022.<br>
                    <b>[May 2022]</b> Started an internship at <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a> @Redmond.<br>
                    <b>[Feb 2022]</b> Received UTokyo-IIS Research Collaboration Initiative Award with <a href="https://twitter.com/tkhkaeio/status/1507297146772799491?s=20&t=0IM8HYvhrWsCrqv9SnAJzw">Oculus Quests</a>!<br>                      
                    <details>
                    <summary>Past updates</summary>
                    <b>[Sep 2021]</b> Started working as a Research Scholar at <a href="http://www.cs.cmu.edu/~kkitani/">Kris Lab</a>, Robotics Institute, CMU.<br>
                    <b>[Sep 2021]</b> Obtained M.A.S. for 1.5 years (<a>Early Graduation</a>), UTokyo.<br>
                    <b>[Jun 2021]</b> Paper <a href="http://arxiv.org/abs/2107.02718">"Domain Adaptation of Hand Segmentation"</a> got accepted to IEEE Access 2021.<br>
                    <b>[Oct 2020]</b> Research proposal got accepted to <a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">JST ACT-X</a>.<br>
                    <b>[Oct 2020]</b> Paper <a href="https://arxiv.org/abs/2003.00187">"Augmented Cyclic Consistency Regularization"</a> got accepted to ICPR2020.<br>
                    <b>[Aug 2020]</b> Started an internship at <a href="https://www.omron.com/sinicx/">OSX</a>, Tokyo.<br>
                    <b>[Apr 2020]</b> Joined <a href="https://www.ut-vision.org/sato-lab/">Sato/Sugano Lab</a> at UTokyo.<br>
                    <b>[Mar 2020]</b> Obtained B.E. for 3 years (<a>Early Graduation</a>) at TokyoTech.<br>
                    <b>[Oct 2019]</b> Gifted NVIDIA RTX 2080Ti from <a href="https://twitter.com/tkhkaeio/status/1186167767415672833?s=20">Yu Darvish</a>, a Japanese MLB player who I respect the most!<br>
                    <b>[Oct 2019]</b> Joined <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Inoue Lab</a> at TokyoTech.<br>
                    </details>
                </p>

                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                    <blockquote style="font-style:italic;">
                    <p>"It is not the strongest of the species that survives, nor the most intelligent. It is the one most <b>adaptable to change</b>." <div style="text-align:right">-Charles Darwin (1809-1882)</div></p>
                    </blockquote>

                    C. Darwin left us with an astute analysis of the nature of intelligence in the open world. This insight informs my endeavors to create robust, flexible, and autonomous machine intelligence that assists in daily activities and augments human capabilities.
                    <!-- <br><br>                       -->
                    In particular, I am curious about adaptive camera systems and visual sensing from moving egocentric cameras. This envisions wider applications in AR/VR interfaces, robotic grasping and teaching, and assistive technologies.                                    
                </p>
                </td>
            </tr>
            </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <!-- <img src="https://user-images.githubusercontent.com/28190044/224896516-0c474c0b-a76a-4de0-b539-fc4a93b048d5.png" width="180" height="180"> -->
                <img src="https://user-images.githubusercontent.com/28190044/228722223-dc127074-61e5-4b37-92dc-058a73780706.gif" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/tbd">
                <papertitle>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, and Cem Keskin
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023<br>
                <a href="https://arxiv.org/abs/tbd">[Paper]</a>
                <a href="https://assemblyhands.github.io/">[Project]</a>
                <label class="open" for="pop-up-4"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-4" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-4">×</label>
                    <div class="bibtex" id="bibtex-23-assemblyhands"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/23-assemblyhands/bibtex.txt';
                    load_text(filename, 'bibtex-23-assemblyhands');
                    </script>
                </div>
                </div>

                <p>
                We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of challenging hand-object interactions from egocentric videos.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/172274533-aaaf7014-a379-4025-a6e2-ad9e96da7b75.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.02257">
                <papertitle>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em>arXiv Preprint</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2206.02257">[Paper]</a>
                <label class="open" for="pop-up-4"><a>[BibTex]</a></label>
                <a href="https://drive.google.com/file/d/1YzlAV4Om0TQfJKhfFMZlFeiTHqHUqAH7/view?usp=share_link">[Slides]</a>
                <input type="checkbox" id="pop-up-4" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-4">×</label>
                    <div class="bibtex" id="bibtex-22-hpe-survey"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/22-hpe-survey/bibtex.txt';
                    load_text(filename, 'bibtex-22-hpe-survey');
                    </script>
                </div>
                </div>

                <p>
                We present a comprehensive analysis of 3D hand pose estimation from the perspective of efficient annotation and learning, covering recent approaches for 3D hand pose annotation and learning methods with limited annotated data.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/159080951-fe11090c-664c-4ce1-bf11-67ae2184b7f9.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2203.08344">
                <papertitle>Domain Adaptive Hand Keypoint and Pixel Localization in the Wild</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, and Yoichi Sato<br>
                <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022<br>
                Invited Workshop Posters at <em><a href="https://sites.google.com/view/hands2022/home?authuser=0">HANDS</a> and <a href="https://sites.google.com/view/egocentric-hand-body-activity/home?authuser=0">HBHA</a>, ECCVW, 2022</em>
                <br>
                <a href="https://arxiv.org/abs/2203.08344">[Paper]</a>
                <a href="./projects/22-hand-ps-da/">[Project]</a>
                <a href="https://drive.google.com/file/d/1Ub_-653uSLYb70YeeXpE67H2MDWOrr1z/view?usp=share_link">[Slides]</a>
                <label class="open" for="pop-up-3"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-3" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-3">×</label>
                    <div class="bibtex" id="bibtex-22-hand-ps-da"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/22-hand-ps-da/bibtex.txt';
                    load_text(filename, 'bibtex-22-hand-ps-da');
                    </script>
                </div>
                </div>
                <p>
                We tackled on domain adaptation of hand keypoint regression and hand segmentation to in-the-wild egocentric videos with new imaging conditions (e.g., Ego4D).
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img
                    src="https://user-images.githubusercontent.com/28190044/156375706-66ba3fe7-25f5-4405-a987-e787b993b362.png"
                    width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2202.13941">
                <papertitle>Background Mixup Data Augmentation for Hand and Object-in-Contact Detection</papertitle>
                </a>
                <br>
                Koya Tango, <b>Takehiko Ohkawa</b>, Ryosuke Furuta, and Yoichi Sato
                <br>
                <em><a href="https://sites.google.com/view/hands2022/home?authuser=0">HANDS</a>, European Conference on Computer Vision Workshops (<b>ECCVW</b>)</em>, 2022

                <br>
                <!-- <a href="xxxxx">project page</a> / -->
                <a href="https://arxiv.org/abs/2202.13941">[Paper]</a>
                <!-- <a href="./projects/20-accr/bibtex.txt">[BibTex]</a> -->
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <p></p>
                <p>
                We propose Background Mixup augmentation that leverages data-mixing regularization for hand-object detection while avoiding unintended effect produced by naive Mixup.
                </p>
            </td>
            </tbody>
            <tbody>
            <!-- <tr onmouseout="idName_stop()" onmouseover="idName_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
            <div class="two" id='id_name'><video  width=100% height=100% muted autoplay loop>
            <source src="images/xxx.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='https://user-images.githubusercontent.com/28190044/106457189-f2d63880-64d1-11eb-9cfa-cc22e79b7809.png' width="160">
            </div>
            <script type="text/javascript">
            function idName_start() {
                document.getElementById('id_name').style.opacity = "1";
            }
            function idName_stop() {
                document.getElementById('id_name').style.opacity = "0";
            }
            idName_stop()
            </script>
        </td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src="https://user-images.githubusercontent.com/28190044/124936861-dcfd0e80-e041-11eb-98bb-64ae613aa74c.gif" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://arxiv.org/abs/2107.02718">
                <papertitle>Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation</papertitle><br>
                </a>
                <b>Takehiko Ohkawa</b>, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato<br>
                <em><b>IEEE Access</b></em>, 2021<br>
                <a href="http://arxiv.org/abs/2107.02718">[Paper]</a>
                <a href="https://ieeexplore.ieee.org/document/9469781">[IEEE Xplore]</a>
                <!-- <a href="https://www.youtube.com/watch?v=xxxx">video</a> -->
                <a href="projects/21-fgsty-cpl/index.html">[Project]</a>
                <a href="https://github.com/ut-vision/FgSty-CPL">[Code & Data]</a>
                <label class="open" for="pop-up-2"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-2" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-2">×</label>
                    <div class="bibtex" id="bibtex-21-fgsty-cpl"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/21-fgsty-cpl/bibtex.txt';
                    load_text(filename, 'bibtex-21-fgsty-cpl');
                    </script>
                </div>
                </div>
                <p>
                We developed a domain adaptation method for hand segmentation, consisting of appearance gap reduction by stylization and learning with pseudo-labels generated by network consensus.
                </p>
            </td>
            </tbody>
            <tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src="https://user-images.githubusercontent.com/28190044/106457189-f2d63880-64d1-11eb-9cfa-cc22e79b7809.png" width="180" height="180">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2003.00187">
                <papertitle>Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image Translation</papertitle>
                </a>
                <br>
                <b>Takehiko Ohkawa</b>, Naoto Inoue, Hirokatsu Kataoka, and Nakamasa Inoue
                <br>
                <em>International Conference on Pattern Recognition (<b>ICPR</b>)</em>, 2020
                <br>
                <!-- <a href="xxxxx">project page</a> / -->
                <a href="https://arxiv.org/abs/2003.00187">[Paper]</a>
                <label class="open" for="pop-up-1"><a>[BibTex]</a></label>
                <input type="checkbox" id="pop-up-1" style="display: none;">
                <div class="overlay">
                <div class="window">
                    <label class="close" for="pop-up-1">×</label>
                    <div class="bibtex" id="bibtex-20-accr"></div>
                    <script>
                    var filename = 'https://tkhkaeio.github.io/projects/20-accr/bibtex.txt';
                    load_text(filename, 'bibtex-20-accr');
                    </script>
                </div>
                </div>
                <p>
                We developed extended consistency regularization for stabilizing the training of image translation models using real, fake, and reconstructed samples.
                </p>
            </td></tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research & Work Experience</heading>
                <p>
                    <b>[Nov 2020 - Present]</b> Research assistant, <a href="https://www.ut-vision.org/sato-lab/">Sato Lab</a>, UTokyo<br>
                    <b>[Apr 2023 - Mar 2024]</b> Research collaboration, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a><br>
                    <b>[Jan 2023 - May 2023]</b> Research internship, <a href="https://www.omron.com/sinicx/">OMRON SINIC X Corp.</a><br>
                    <b>[May 2022 - Nov 2022]</b> Research internship, <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a> @Redmond.<br>
                    <b>[Sep 2021 - Mar 2022]</b> Research scholar, <a href="http://www.cs.cmu.edu/~kkitani/">Kris Lab</a>, CMU<br>
                    <b>[Aug 2020 - Aug 2021]</b> Research internship, <a href="https://www.omron.com/sinicx/">OMRON SINIC X Corp.</a><br>
                    <b>[Oct 2019 - May 2020]</b> Research internship, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.neuralpocket.com%2Fen%2F&sa=D&sntz=1&usg=AFQjCNHPYi2Zyj7HCozNbgGoqOxrXLViow">Neural Pocket Inc.</a><br>
                    <b>[Aug 2019 - Mar 2020]</b> Research assistant, <a href="https://www.google.com/url?q=https%3A%2F%2Fmmai.tech%2F%23home&sa=D&sntz=1&usg=AFQjCNHFzzL1iApg76zsB8hBWNS-fGyrtg">Inoue Lab</a>, TokyoTech<br>
                    <b>[Aug 2019 - Sep 2019]</b> Engineering internship, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.teamlab.art%2F&sa=D&sntz=1&usg=AFQjCNGtI-cRUBi2GvUXaa0A-WPIxxyA6w">teamLab Inc.</a> <br>
                    <b>[Dec 2017 - Nov 2018]</b> Research internship, <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.cross-compass.com%2Fen%2Ffront-page%2F&sa=D&sntz=1&usg=AFQjCNHBhiK40Hg3ganunGUZLZIzIrUoXg">Cross Compass Ltd. </a>
                </p>
                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Awards & Grants</heading>
                <p>                      
                    JSPS Research Fellowship for Young Scientists (DC1), 2022-2025<br>
                    Microsoft Research Asia Collaborative Research Program D-CORE, 2023<br>
                    JST ACT-X Acceleration Phase of "Frontier of Mathematics and Information Science", 2023<br>
                    JST ACT-X "Frontier of Mathematics and Information Science", 2020-2023<br>
                    UTokyo-IIS Travel Grant for International Research Meetings, 2022<br>
                    JASSO Scholarship for Excellent Master Students at UTokyo, 2021<br>
                    UTokyo-IIS Research Collaboration Initiative Award, 2021<br>
                    MIRU Student Encouragement Award, 2021<br>
                    PRMU Best Presentation of the Month, 2020<br>
                    JEES/Softbank AI Scholarship, 2020<br>
                    Tokio Marine Kagami Memorial Foundation Scholarship, 2018-2020<br>
                </p>
                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <!-- <heading>Professional Services & Activities</heading> -->
        <heading>Activities</heading>
            <!-- <p>
            <b>Reviewers:</b> 
            <ul>
                <li> European Conference on Computer Vision: ECCV2022</li>
                <li> Meeting on Image Recognition and Understanding: MIRU2022</li>
            </ul>
            </p> -->
            <p>
            <b>Lab Visits:</b>
            <ul>
                <li>2022: 
                        Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>@ETH,
                        Dr. <a href="https://de.linkedin.com/in/fadime-sener-674064156">Fadime Sener</a>, 
                        Dr. <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a> & Dr. <a href="https://btekin.github.io/">Buğra Tekin</a> @Meta (Zurich),
                        Dr. <a href="http://robotics.naist.jp/members/j-taka/">Jun Takamatsu</a>@Microsoft (Redmond), Dr. <a href="https://radmahdi.github.io/Home.html">Mahdi Rad</a>@Microsoft (Zurich).
                        </li>
                <li>2021: Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> @CMU</li>
                <li>2019: Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu">Jiwen Lu</a> @Tsinghua Univ., 
                        Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ks.c.titech.ac.jp%2F~shinoda%2Findex.html&sa=D&sntz=1&usg=AFQjCNHYXCsAe_UkdMMRg7SxTCzSsS7-Cg">Koichi Shinoda</a> @TokyoTech, 
                        Dr. <a href="http://hirokatsukataoka.net">Hirokatsu Kataoka</a> @AIST, 
                        Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.cbi.c.titech.ac.jp%2Fsekijima.html&sa=D&sntz=1&usg=AFQjCNFSCKmGj9FkReAzjJiWYN2qzV5NIA">Masakazu Sekijima</a>@TokyoTech</li>
                <li>2018: Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.chokkan.org%2Findex.en.html&sa=D&sntz=1&usg=AFQjCNERO4X90fx_xrtsQEGLiczXnaWopA">Naoaki Okazaki</a> @TokyoTech, 
                        Prof. <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ymatsuo.com%2F&sa=D&sntz=1&usg=AFQjCNGTFtL0e4HnNWViMbXcqxkygBoz5g">Yutaka Matsuo</a> @UTokyo</li>
            </ul>
            </p>
            <p>
            <b>Conference Participation:</b>
            <ul>
                <li>2023: CVPR (Vancouver, Canada) </li>
                <li>2022: ECCV (Tel-Aviv, Israel), SIGGRAPH (Vancouver, Canada), WACV (Hawaii, USA)</li>
                <li>2019: ICCV (Soul, Korea)</li>
            </ul>
        <!-- Invited Talks: <br> -->
            </p>
        </td>
        </tr>
    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:0px">
                <br>
                <p
                    style="text-align:center;font-size:small;">
                    <script type="text/javascript"
                    id="clustrmaps"
                    src="//cdn.clustrmaps.com/map_v2.js?cl=c9c1c1&w=300&t=n&d=j0j_3rEh4HCvS5DIxlRJ3P9J8yp0mtUeDLZ6Z6I2NJ8&co=ffffff&cmo=ea8d7a&cmn=1772d0&ct=000000"></script>
                </p>
                <p
                    style="text-align:right;font-size:small;">
                    © Takehiko Ohkawa 2023 /
                    Design: <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>
                </p>
                </td>
            </tr>
            </tbody></table>
        <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-162031573-1', 'auto');
        ga('send', 'pageview');
    </script>
        </td>
    </tr>
    </tbody></table><iframe frameborder="0" scrolling="no"
    style="border: 0px; display: none; background-color: transparent;"></iframe>
<div id="GOOGLE_INPUT_CHEXT_FLAG" input="null"
    input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}"
    style="display: none;"></div>
</body>

</html>
